{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T20:48:30.550283Z",
     "start_time": "2025-09-30T20:48:30.179561Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import random\n",
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.ndimage import label\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\", over=\"ignore\", under=\"ignore\")   # also: divide=\"ignore\", over=\"ignore\", under=\"ignore\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=r\"invalid value encountered in add\"\n",
    ")\n",
    "\n",
    "from scipy.special import zeta  # For normalization\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage import find_objects\n",
    "from scipy.ndimage import label\n",
    "import powerlaw\n",
    "warnings.filterwarnings('ignore')\n",
    "import cma"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:48:30.556536Z",
     "start_time": "2025-09-30T20:48:30.552921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fit_power_law_numpy(data):\n",
    "    \"\"\"\n",
    "    Fits a power-law distribution to discrete data using the Clauset, Shalizi, Newman method.\n",
    "\n",
    "    Returns a dictionary containing fit parameters and data for plotting.\n",
    "    \"\"\"\n",
    "    # --- The first part of the function (finding the best alpha and xmin) is the same ---\n",
    "    possible_xmins = np.unique(data)\n",
    "    possible_xmins = possible_xmins[possible_xmins < np.max(possible_xmins)]\n",
    "\n",
    "    if len(possible_xmins) < 2:\n",
    "        return {'alpha': 1.0, 'xmin': 1, 'ks_distance': 1.0, 'fit_successful': False}\n",
    "\n",
    "    ks_distances = []\n",
    "    alphas = []\n",
    "\n",
    "    for xmin_candidate in possible_xmins:\n",
    "        tail_data = data[data >= xmin_candidate]\n",
    "        n_tail = len(tail_data)\n",
    "        if n_tail < 10:\n",
    "            ks_distances.append(np.inf)\n",
    "            alphas.append(1.0)\n",
    "            continue\n",
    "\n",
    "        alpha_mle = 1.0 + n_tail / np.sum(np.log(tail_data / (xmin_candidate - 0.5)))\n",
    "        alphas.append(alpha_mle)\n",
    "\n",
    "        empirical_cdf = np.arange(1, n_tail + 1) / n_tail\n",
    "        unique_tail_data, counts = np.unique(tail_data, return_counts=True)\n",
    "\n",
    "        # This part has been simplified for clarity, the logic is the same\n",
    "        theoretical_cdf = np.cumsum((unique_tail_data**-alpha_mle) / zeta(alpha_mle, xmin_candidate))\n",
    "        cdf_map = dict(zip(unique_tail_data, theoretical_cdf))\n",
    "        theoretical_cdf_full = np.array([cdf_map[val] for val in tail_data])\n",
    "\n",
    "        ks_distance = np.max(np.abs(empirical_cdf - theoretical_cdf_full))\n",
    "        ks_distances.append(ks_distance)\n",
    "\n",
    "    if np.all(np.isinf(ks_distances)):\n",
    "        return {'alpha': 1.0, 'xmin': 1, 'ks_distance': 1.0, 'fit_successful': False}\n",
    "\n",
    "    best_idx = np.argmin(ks_distances)\n",
    "    best_xmin = possible_xmins[best_idx]\n",
    "    best_alpha = alphas[best_idx]\n",
    "    min_ks_distance = ks_distances[best_idx]\n",
    "\n",
    "    # --- NEW: Calculate data for plotting ---\n",
    "\n",
    "    # 1. Empirical PDF (Probability Density Function)\n",
    "    unique_sizes, counts = np.unique(data, return_counts=True)\n",
    "    empirical_pdf_y = counts / len(data)\n",
    "    empirical_pdf_x = unique_sizes\n",
    "\n",
    "    # 2. Theoretical PDF (the fitted line)\n",
    "    # The line should only be drawn for x >= xmin\n",
    "    fit_pdf_x = np.arange(best_xmin, np.max(data) + 1, dtype=np.float64)\n",
    "    # Normalization constant C = 1 / zeta(alpha, xmin)\n",
    "    normalization_const = 1 / zeta(best_alpha, best_xmin)\n",
    "    fit_pdf_y = normalization_const * (fit_pdf_x ** -best_alpha)\n",
    "\n",
    "    # 3. Package everything into a dictionary\n",
    "    results = {\n",
    "        'alpha': best_alpha,\n",
    "        'xmin': best_xmin,\n",
    "        'ks_distance': min_ks_distance,\n",
    "        'fit_successful': True,\n",
    "        'empirical_pdf_x': empirical_pdf_x,\n",
    "        'empirical_pdf_y': empirical_pdf_y,\n",
    "        'fit_pdf_x': fit_pdf_x,\n",
    "        'fit_pdf_y': fit_pdf_y\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "4c493e6b4131ae8d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:48:30.768183Z",
     "start_time": "2025-09-30T20:48:30.608679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from skimage import measure\n",
    "import powerlaw\n",
    "\n",
    "# -------------------------\n",
    "# Helpers copied from paper\n",
    "# -------------------------\n",
    "def KSdist(theoretical_pdf, empirical_pdf):\n",
    "    theoretical_pdf = np.asarray(theoretical_pdf, dtype=np.float64)\n",
    "    empirical_pdf   = np.asarray(empirical_pdf,   dtype=np.float64)\n",
    "    if theoretical_pdf.sum() > 0:\n",
    "        theoretical_pdf = theoretical_pdf / theoretical_pdf.sum()\n",
    "    if empirical_pdf.sum() > 0:\n",
    "        empirical_pdf   = empirical_pdf / empirical_pdf.sum()\n",
    "    return float(np.max(np.abs(np.cumsum(theoretical_pdf) - np.cumsum(empirical_pdf))))\n",
    "\n",
    "def getdict_cluster_size(arr1d):\n",
    "    cluster_dict = {}\n",
    "    current_number = None\n",
    "    for a in arr1d:\n",
    "        if current_number == a:\n",
    "            cluster_dict[a][-1] = cluster_dict[a][-1] + 1\n",
    "        else:\n",
    "            current_number = a\n",
    "            if a in cluster_dict:\n",
    "                cluster_dict[a].append(1)\n",
    "            else:\n",
    "                cluster_dict[a] = [1]\n",
    "    return cluster_dict\n",
    "\n",
    "def getarray_avalanche_size(x, value):\n",
    "    list_avalance_size = []\n",
    "    if value in x:\n",
    "        x0size, x1size = x.shape\n",
    "        for i in range(x0size):\n",
    "            if value in x[i, :]:\n",
    "                list_avalance_size.extend(getdict_cluster_size(x[i, :])[value])\n",
    "    return np.array(list_avalance_size)\n",
    "\n",
    "def getarray_avalanche_duration(x, value):\n",
    "    list_avalance_duration = []\n",
    "    if value in x:\n",
    "        x0size, x1size = x.shape\n",
    "        for i in range(x1size):\n",
    "            if value in x[:, i]:\n",
    "                list_avalance_duration.extend(getdict_cluster_size(x[:, i])[value])\n",
    "    return np.array(list_avalance_duration)\n",
    "\n",
    "def norm_ksdist(ksdist, smooth=1):\n",
    "    # exp(- α_D * (0.9*min(mean of first 3, mean of last 3) + 0.1*overall mean))\n",
    "    return float(np.exp(-smooth * (0.9 * min(np.mean(ksdist[:3]), np.mean(ksdist[3:])) + 0.1 * np.mean(ksdist))))\n",
    "\n",
    "def norm_linscore(linscore):\n",
    "    return float(np.mean(linscore))\n",
    "\n",
    "def norm_coef(coef):\n",
    "    return float(-np.mean(coef))\n",
    "\n",
    "def sigmoid(x, smooth=0.01):\n",
    "    return float(1. / (1. + np.exp(-x * smooth)))\n",
    "\n",
    "def norm_R(R_list):\n",
    "    return sigmoid(0.9 * max(np.mean(R_list[:3]), np.mean(R_list[3:])) + 0.1 * np.mean(R_list), smooth=0.01)\n",
    "\n",
    "def normalize_avalanche_pdf_size(mask_avalanche_s_0_bc, mask_avalanche_d_0_bc,\n",
    "                                 mask_avalanche_t_0_bc, mask_avalanche_s_1_bc,\n",
    "                                 mask_avalanche_d_1_bc, mask_avalanche_t_1_bc,\n",
    "                                 width, timesteps):\n",
    "    norm_avalanche_pdf_size_s_0 = np.sum(mask_avalanche_s_0_bc) / width\n",
    "    norm_avalanche_pdf_size_d_0 = np.sum(mask_avalanche_d_0_bc) / timesteps\n",
    "    norm_avalanche_pdf_size_t_0 = np.sum(mask_avalanche_t_0_bc) / (timesteps * width)\n",
    "    norm_avalanche_pdf_size_s_1 = np.sum(mask_avalanche_s_1_bc) / width\n",
    "    norm_avalanche_pdf_size_d_1 = np.sum(mask_avalanche_d_1_bc) / timesteps\n",
    "    norm_avalanche_pdf_size_t_1 = np.sum(mask_avalanche_t_1_bc) / (timesteps * width)\n",
    "\n",
    "    mean_avalanche_pdf_size = np.mean([\n",
    "        norm_avalanche_pdf_size_s_0, norm_avalanche_pdf_size_d_0, norm_avalanche_pdf_size_t_0,\n",
    "        norm_avalanche_pdf_size_s_1, norm_avalanche_pdf_size_d_1, norm_avalanche_pdf_size_t_1\n",
    "    ])\n",
    "\n",
    "    max_avalanche_pdf_size = max(\n",
    "        np.mean([norm_avalanche_pdf_size_s_0, norm_avalanche_pdf_size_d_0, norm_avalanche_pdf_size_t_0]),\n",
    "        np.mean([norm_avalanche_pdf_size_s_1, norm_avalanche_pdf_size_d_1, norm_avalanche_pdf_size_t_1])\n",
    "    )\n",
    "\n",
    "    return float(np.tanh(5 * (0.9 * max_avalanche_pdf_size + 0.1 * mean_avalanche_pdf_size)))\n",
    "\n",
    "def calculate_comparison_ratio(data):\n",
    "    fit = powerlaw.Fit(data, xmin=1, discrete=True)\n",
    "    R_exp, p_exp = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "    return float(R_exp if p_exp < 0.1 else 0.0)\n",
    "\n",
    "# -------------------------\n",
    "# Labeling exactly as paper\n",
    "# -------------------------\n",
    "def get_numbered_avalanches(x, value):\n",
    "    x_value = (x == value).astype(np.int8)\n",
    "    numbered_avalanches_x = measure.label(x_value, background=0)\n",
    "    # Merge wrap-around labels touching both sides\n",
    "    for i in range(numbered_avalanches_x.shape[0]):\n",
    "        if (numbered_avalanches_x[i, 0] != 0 and\n",
    "            numbered_avalanches_x[i, -1] != 0 and\n",
    "            numbered_avalanches_x[i, 0] != numbered_avalanches_x[i, -1]):\n",
    "            numbered_avalanches_x[numbered_avalanches_x == numbered_avalanches_x[i, -1]] = numbered_avalanches_x[i, 0]\n",
    "    return numbered_avalanches_x\n",
    "\n",
    "def getarray_avalanche_size_duration_total(x, value):\n",
    "    numbered_avalanches = get_numbered_avalanches(x, value)\n",
    "    number_of_avalanches = int(np.max(numbered_avalanches))\n",
    "    avalanche_size, avalanche_duration, avalanche_total = [], [], []\n",
    "    for avalanche_number in range(1, number_of_avalanches + 1):\n",
    "        avalanche = np.argwhere(numbered_avalanches == avalanche_number)\n",
    "        if len(avalanche) > 0:\n",
    "            avalanche_duration.append(len(np.unique(avalanche[:, 0])))\n",
    "            avalanche_size.append(len(np.unique(avalanche[:, 1])))\n",
    "            avalanche_total.append(len(avalanche))\n",
    "    return avalanche_size, avalanche_duration, avalanche_total\n",
    "\n",
    "# --------------------------------------\n",
    "# Literal port of evaluate_result(...) —\n",
    "# — but parameterized by width/timesteps\n",
    "# --------------------------------------\n",
    "def evaluate_result_port(ca_result_bin):\n",
    "    \"\"\"\n",
    "    ca_result_bin: 2D uint8/bool array (timesteps x width) with values {0,1}\n",
    "    Returns: fitness (positive), val_dict (same fields as paper)\n",
    "    \"\"\"\n",
    "    timesteps, width = ca_result_bin.shape\n",
    "\n",
    "    # Collect avalanches for both states and all three measures\n",
    "    avalanche_s_0, avalanche_d_0, avalanche_t_0 = getarray_avalanche_size_duration_total(ca_result_bin, 0)\n",
    "    avalanche_s_1, avalanche_d_1, avalanche_t_1 = getarray_avalanche_size_duration_total(ca_result_bin, 1)\n",
    "\n",
    "    # Bin counts, drop zero bin, require >5 samples as in paper paths\n",
    "    avalanche_s_0_bc = np.bincount(avalanche_s_0)[1:] if len(avalanche_s_0) > 5 else np.array([])\n",
    "    avalanche_d_0_bc = np.bincount(avalanche_d_0)[1:] if len(avalanche_d_0) > 5 else np.array([])\n",
    "    avalanche_t_0_bc = np.bincount(avalanche_t_0)[1:] if len(avalanche_t_0) > 5 else np.array([])\n",
    "\n",
    "    avalanche_s_1_bc = np.bincount(avalanche_s_1)[1:] if len(avalanche_s_1) > 5 else np.array([])\n",
    "    avalanche_d_1_bc = np.bincount(avalanche_d_1)[1:] if len(avalanche_d_1) > 5 else np.array([])\n",
    "    avalanche_t_1_bc = np.bincount(avalanche_t_1)[1:] if len(avalanche_t_1) > 5 else np.array([])\n",
    "\n",
    "    # Normalize to PDFs (paper does this)\n",
    "    def _to_pdf(bc):\n",
    "        bc = np.asarray(bc, dtype=np.float64)\n",
    "        s = bc.sum()\n",
    "        return bc / s if s > 0 else bc\n",
    "\n",
    "    avalanche_s_0_bc = _to_pdf(avalanche_s_0_bc)\n",
    "    avalanche_d_0_bc = _to_pdf(avalanche_d_0_bc)\n",
    "    avalanche_t_0_bc = _to_pdf(avalanche_t_0_bc)\n",
    "    avalanche_s_1_bc = _to_pdf(avalanche_s_1_bc)\n",
    "    avalanche_d_1_bc = _to_pdf(avalanche_d_1_bc)\n",
    "    avalanche_t_1_bc = _to_pdf(avalanche_t_1_bc)\n",
    "\n",
    "    # Masks for non-zero bins (paper uses these)\n",
    "    mask_avalanche_s_0_bc = avalanche_s_0_bc > 0\n",
    "    mask_avalanche_d_0_bc = avalanche_d_0_bc > 0\n",
    "    mask_avalanche_t_0_bc = avalanche_t_0_bc > 0\n",
    "    mask_avalanche_s_1_bc = avalanche_s_1_bc > 0\n",
    "    mask_avalanche_d_1_bc = avalanche_d_1_bc > 0\n",
    "    mask_avalanche_t_1_bc = avalanche_t_1_bc > 0\n",
    "\n",
    "    # log10 PDFs, zeros where masked out (exactly like the paper)\n",
    "    def _log_masked(pdf, mask):\n",
    "        logp = np.zeros_like(pdf)\n",
    "        nz = mask\n",
    "        logp[nz] = np.log10(pdf[nz])\n",
    "        return logp\n",
    "\n",
    "    log_avalanche_s_0_bc = _log_masked(avalanche_s_0_bc, mask_avalanche_s_0_bc)\n",
    "    log_avalanche_d_0_bc = _log_masked(avalanche_d_0_bc, mask_avalanche_d_0_bc)\n",
    "    log_avalanche_t_0_bc = _log_masked(avalanche_t_0_bc, mask_avalanche_t_0_bc)\n",
    "    log_avalanche_s_1_bc = _log_masked(avalanche_s_1_bc, mask_avalanche_s_1_bc)\n",
    "    log_avalanche_d_1_bc = _log_masked(avalanche_d_1_bc, mask_avalanche_d_1_bc)\n",
    "    log_avalanche_t_1_bc = _log_masked(avalanche_t_1_bc, mask_avalanche_t_1_bc)\n",
    "\n",
    "    # --- Hard gate: need >5 non-zero bins among first 10, for all six dists ---\n",
    "    def _left10_nonzero(mask):\n",
    "        return int(np.sum(mask[:10]))\n",
    "\n",
    "    if not ( _left10_nonzero(mask_avalanche_s_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_d_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_t_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_s_1_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_d_1_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_t_1_bc) > 5 ):\n",
    "        # Paper returns effectively zero fitness if this fails\n",
    "        return 0.0, {\n",
    "            \"norm_ksdist_res\": 0.0,\n",
    "            \"norm_coef_res\":   0.0,\n",
    "            \"norm_unique_states\": 0.0,\n",
    "            \"norm_avalanche_pdf_size\": 0.0,\n",
    "            \"norm_linscore_res\": 0.0,\n",
    "            \"norm_R_res\": 0.0,\n",
    "            \"fitness\": 0.0,\n",
    "        }\n",
    "\n",
    "    # --- Fit log–log lines (unweighted R^2), then refit with sample_weight for first 10 bins ---\n",
    "    def _fit_lin(pdf, log_pdf, mask):\n",
    "        xs = np.log10(np.arange(1, len(pdf) + 1)[mask]).reshape(-1, 1)\n",
    "        ys = log_pdf[mask]\n",
    "        if xs.shape[0] == 0:\n",
    "            return None, 0.0\n",
    "        lr = LinearRegression().fit(xs, ys)\n",
    "        r2 = lr.score(xs, ys)\n",
    "        # re-fit with weights that are 1 for bins <10, else 0 (paper’s weighted fit)\n",
    "        idxs_all = np.arange(len(pdf))[mask]\n",
    "        w = np.array([1 if idx < 10 else 0 for idx in idxs_all], dtype=float)\n",
    "        lr_w = LinearRegression().fit(xs, ys, sample_weight=w)\n",
    "        return (lr, lr_w)\n",
    "\n",
    "    fits = []\n",
    "    for pdf, logp, mask in [\n",
    "        (avalanche_s_0_bc, log_avalanche_s_0_bc, mask_avalanche_s_0_bc),\n",
    "        (avalanche_d_0_bc, log_avalanche_d_0_bc, mask_avalanche_d_0_bc),\n",
    "        (avalanche_t_0_bc, log_avalanche_t_0_bc, mask_avalanche_t_0_bc),\n",
    "        (avalanche_s_1_bc, log_avalanche_s_1_bc, mask_avalanche_s_1_bc),\n",
    "        (avalanche_d_1_bc, log_avalanche_d_1_bc, mask_avalanche_d_1_bc),\n",
    "        (avalanche_t_1_bc, log_avalanche_t_1_bc, mask_avalanche_t_1_bc),\n",
    "    ]:\n",
    "        lr, lr_w = _fit_lin(pdf, logp, mask)\n",
    "        fits.append((pdf, logp, mask, lr, lr_w))\n",
    "\n",
    "    # R^2 list from the unweighted fits\n",
    "    linscore_list = []\n",
    "    for pdf, logp, mask, lr, lr_w in fits:\n",
    "        xs = np.log10(np.arange(1, len(pdf) + 1)[mask]).reshape(-1, 1)\n",
    "        ys = logp[mask]\n",
    "        linscore_list.append(lr.score(xs, ys))\n",
    "\n",
    "    # Theoretical PDFs from the weighted fits (full support, not only masked bins)\n",
    "    def _theor_from_fit(lr_w, L):\n",
    "        xs_full = np.log10(np.arange(1, L + 1).reshape(-1, 1))\n",
    "        pred = lr_w.predict(xs_full)  # log10(pdf)\n",
    "        theor = np.power(10.0, pred)\n",
    "        return theor\n",
    "\n",
    "    theor_avalanche_s_0_bc = _theor_from_fit(fits[0][4], len(avalanche_s_0_bc))\n",
    "    theor_avalanche_d_0_bc = _theor_from_fit(fits[1][4], len(avalanche_d_0_bc))\n",
    "    theor_avalanche_t_0_bc = _theor_from_fit(fits[2][4], len(avalanche_t_0_bc))\n",
    "    theor_avalanche_s_1_bc = _theor_from_fit(fits[3][4], len(avalanche_s_1_bc))\n",
    "    theor_avalanche_d_1_bc = _theor_from_fit(fits[4][4], len(avalanche_d_1_bc))\n",
    "    theor_avalanche_t_1_bc = _theor_from_fit(fits[5][4], len(avalanche_t_1_bc))\n",
    "\n",
    "    # KS against the fitted curves\n",
    "    ksdist_list = [\n",
    "        KSdist(theor_avalanche_s_0_bc, avalanche_s_0_bc),\n",
    "        KSdist(theor_avalanche_d_0_bc, avalanche_d_0_bc),\n",
    "        KSdist(theor_avalanche_t_0_bc, avalanche_t_0_bc),\n",
    "        KSdist(theor_avalanche_s_1_bc, avalanche_s_1_bc),\n",
    "        KSdist(theor_avalanche_d_1_bc, avalanche_d_1_bc),\n",
    "        KSdist(theor_avalanche_t_1_bc, avalanche_t_1_bc),\n",
    "    ]\n",
    "\n",
    "    # Coefs (slopes) from the weighted fits\n",
    "    coef_list = [\n",
    "        fits[0][4].coef_[0], fits[1][4].coef_[0], fits[2][4].coef_[0],\n",
    "        fits[3][4].coef_[0], fits[4][4].coef_[0], fits[5][4].coef_[0],\n",
    "    ]\n",
    "\n",
    "    # B (coverage of first-10 bins), exactly as in paper's normalize function\n",
    "    norm_avalanche_pdf_size = normalize_avalanche_pdf_size(\n",
    "        fits[0][2], fits[1][2], fits[2][2], fits[3][2], fits[4][2], fits[5][2],\n",
    "        width=width, timesteps=timesteps\n",
    "    )\n",
    "\n",
    "    # Map components to [0,1] in the same way\n",
    "    norm_linscore_res   = norm_linscore(linscore_list)\n",
    "    norm_ksdist_res     = norm_ksdist(ksdist_list)\n",
    "    norm_coef_res       = norm_coef(coef_list)\n",
    "    norm_unique_states  = (np.unique(ca_result_bin, axis=0).shape[0]) / float(width)\n",
    "\n",
    "    # Final fitness (positive)\n",
    "    fitness = (norm_ksdist_res ** 2) + norm_unique_states + norm_avalanche_pdf_size + (norm_linscore_res ** 2)\n",
    "\n",
    "    # Optional L̂ term (power law vs exponential); only if fitness > 3.0, like the paper\n",
    "    norm_R_res = 0.0\n",
    "    if fitness > 3.0:\n",
    "        R_list = [\n",
    "            calculate_comparison_ratio(getarray_avalanche_size(ca_result_bin, 0)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_duration(ca_result_bin, 0)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size_duration_total(ca_result_bin, 0)[2]),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size(ca_result_bin, 1)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_duration(ca_result_bin, 1)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size_duration_total(ca_result_bin, 1)[2]),\n",
    "        ]\n",
    "        norm_R_res = norm_R(R_list)\n",
    "        fitness += norm_R_res\n",
    "\n",
    "    val_dict = {\n",
    "        \"norm_ksdist_res\":      float(norm_ksdist_res),\n",
    "        \"norm_coef_res\":        float(norm_coef_res),\n",
    "        \"norm_unique_states\":   float(norm_unique_states),\n",
    "        \"norm_avalanche_pdf_size\": float(norm_avalanche_pdf_size),\n",
    "        \"norm_linscore_res\":    float(norm_linscore_res),\n",
    "        \"norm_R_res\":           float(norm_R_res),\n",
    "        \"fitness\":              float(fitness),\n",
    "    }\n",
    "    return float(fitness), val_dict\n",
    "\n",
    "# --------------------------------------\n",
    "# Public entry-point for your CEM\n",
    "# --------------------------------------\n",
    "\n"
   ],
   "id": "7ef6990ca9013f17",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:48:30.785051Z",
     "start_time": "2025-09-30T20:48:30.775551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _get_avalanches(matrix, state):\n",
    "    \"\"\"\n",
    "    [Corrected Version - Reflects Figure 1's definitions]\n",
    "    Finds all spatiotemporal avalanches and calculates their size, duration, and area\n",
    "    according to the specific definitions in the Pontes-Filho et al. paper.\n",
    "\n",
    "    - size: Number of unique spatial cells (columns) affected.\n",
    "    - duration: Number of time-steps the avalanche lasts.\n",
    "    - area: Total number of spatiotemporal cells in the cluster.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): The binarized (T, N) state matrix.\n",
    "        state (int): The state to search for (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists: (sizes, durations, areas).\n",
    "    \"\"\"\n",
    "    T, N = matrix.shape\n",
    "    state_matrix = (matrix == state)\n",
    "\n",
    "    structure = np.ones((3, 3), dtype=int)\n",
    "    labeled_matrix, num_avalanches = label(state_matrix, structure=structure)\n",
    "\n",
    "    if num_avalanches == 0:\n",
    "        return [], [], []\n",
    "\n",
    "    # --- Area Calculation (total cells in cluster) ---\n",
    "    # This part was already correct.\n",
    "    # np.bincount is the most efficient way to get the size of each labeled region.\n",
    "    # We discard the count for label 0, which is the background.\n",
    "    areas = np.bincount(labeled_matrix.ravel())[1:]\n",
    "\n",
    "    # --- Duration Calculation (temporal extent) ---\n",
    "    # This part was also correct.\n",
    "    # find_objects gives us the bounding box for each labeled avalanche.\n",
    "    locations = find_objects(labeled_matrix)\n",
    "    durations = [loc[0].stop - loc[0].start for loc in locations]\n",
    "\n",
    "    # --- Size Calculation (unique spatial cells affected) ---\n",
    "    # This is the new, corrected logic.\n",
    "\n",
    "    # Create an array where each element is its own column index.\n",
    "    # e.g., for N=4, it's [[0,1,2,3], [0,1,2,3], ...]\n",
    "    cols_grid = np.tile(np.arange(N), (T, 1))\n",
    "\n",
    "    # Flatten both the labels and the column indices.\n",
    "    labels_flat = labeled_matrix.ravel()\n",
    "    cols_flat = cols_grid.ravel()\n",
    "\n",
    "    # We only care about cells that are part of an avalanche (label > 0).\n",
    "    mask = labels_flat > 0\n",
    "    active_labels = labels_flat[mask]\n",
    "    active_cols = cols_flat[mask]\n",
    "\n",
    "    # Create a 2D array of [label, column] pairs.\n",
    "    label_col_pairs = np.stack([active_labels, active_cols], axis=1)\n",
    "\n",
    "    # Find the unique pairs. This is the key step.\n",
    "    # For each avalanche, this effectively lists each column it touches exactly once.\n",
    "    unique_label_col_pairs = np.unique(label_col_pairs, axis=0)\n",
    "\n",
    "    # Now, count how many unique columns belong to each label.\n",
    "    # The first column of unique_label_col_pairs contains the labels.\n",
    "    # By counting the occurrences of each label ID, we get the 'size'.\n",
    "    unique_labels, sizes = np.unique(unique_label_col_pairs[:, 0], return_counts=True)\n",
    "\n",
    "    # We need to ensure the sizes array is correctly ordered and sized in case some\n",
    "    # label IDs were skipped (though this is unlikely with np.unique).\n",
    "    # We create a zero-filled array and place the counts at the correct index.\n",
    "    final_sizes = np.zeros(num_avalanches, dtype=int)\n",
    "    # unique_labels are 1-based, so we subtract 1 for 0-based indexing.\n",
    "    final_sizes[unique_labels.astype(int) - 1] = sizes\n",
    "\n",
    "    return final_sizes.tolist(), durations, areas.tolist()\n",
    "def _calculate_metrics_from_dist(distribution, num_bins=10):\n",
    "    \"\"\"\n",
    "    [Corrected Version 4]\n",
    "    Calculates R^2 and alpha by fitting a line to the log-log plot of the\n",
    "    PROBABILITY DENSITY, which correctly handles logarithmic binning.\n",
    "    This will produce the correct positive alpha.\n",
    "    \"\"\"\n",
    "    total_avalanches = len(distribution)\n",
    "    results = {\n",
    "        'R2': 0.0, 'B': 0.0, 'alpha': 0.0,\n",
    "        'empirical_x': None, 'empirical_y': None,\n",
    "        'fit_x': None, 'fit_y': None\n",
    "    }\n",
    "\n",
    "    if total_avalanches < 20:\n",
    "        return results\n",
    "\n",
    "    # 1. Empirical PDF for plotting the scattered blue data.\n",
    "    # We will plot density here as well for consistency.\n",
    "    unique_vals, counts = np.unique(distribution, return_counts=True)\n",
    "    # This is a PMF (Probability Mass Function), not a density, but is standard for scatter plots.\n",
    "    probabilities = counts / total_avalanches\n",
    "    results['empirical_x'] = unique_vals\n",
    "    results['empirical_y'] = probabilities\n",
    "\n",
    "    # 2. Binned data for regression\n",
    "    min_val, max_val = np.min(distribution), np.max(distribution)\n",
    "    if min_val <= 0 or min_val == max_val:\n",
    "        return results # log scale requires positive values\n",
    "\n",
    "    bins = np.logspace(np.log10(min_val), np.log10(max_val), num_bins + 1)\n",
    "    hist_counts, bin_edges = np.histogram(distribution, bins=bins)\n",
    "\n",
    "    # --- CRITICAL FIX: Calculate Probability Density ---\n",
    "\n",
    "    # Calculate the width of each bin\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "\n",
    "    # Calculate density, avoiding division by zero\n",
    "    with warnings.catch_warnings(): # Suppress 'invalid value encountered in true_divide'\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        densities = hist_counts / (total_avalanches * bin_widths)\n",
    "\n",
    "    # --- Perform Regression on Density ---\n",
    "    non_zero_mask = densities > 0\n",
    "    densities_for_fit = densities[non_zero_mask]\n",
    "\n",
    "    if len(densities_for_fit) < 3:\n",
    "        return results\n",
    "\n",
    "    bin_centers = ((bin_edges[:-1] + bin_edges[1:]) / 2)[non_zero_mask]\n",
    "\n",
    "    log_x = np.log10(bin_centers)\n",
    "    log_y = np.log10(densities_for_fit)\n",
    "\n",
    "    slope, intercept, r_value, _, _ = linregress(log_x, log_y)\n",
    "\n",
    "    # 3. Store the calculated metrics\n",
    "    results['R2'] = r_value**2 if np.isfinite(r_value) else 0.0\n",
    "    results['B'] = np.sum(hist_counts > 0) / num_bins\n",
    "    # The slope of the density plot is -alpha, so alpha = -slope\n",
    "    results['alpha'] = -slope\n",
    "\n",
    "    # 4. Generate the fitted line for plotting\n",
    "    fit_x_coords = bin_centers\n",
    "\n",
    "    # The line is y = 10^(m*log10(x) + c) in density space\n",
    "    predicted_log_y_density = slope * np.log10(fit_x_coords) + intercept\n",
    "    predicted_y_density = 10**predicted_log_y_density\n",
    "\n",
    "    results['fit_x'] = fit_x_coords\n",
    "\n",
    "    # The y-values of the fit line are densities. We need to convert them to\n",
    "    # probabilities to match the y-axis of the empirical plot (P(x)).\n",
    "    # Prob ~ Density * BinWidth. We use the average bin width for simplicity.\n",
    "    avg_bin_width = np.mean(bin_widths)\n",
    "    results['fit_y'] = predicted_y_density * avg_bin_width * 10 # Heuristic scaling factor for better visualization\n",
    "\n",
    "    # A better approach for the fit line is to plot it in density space, and the empirical data too.\n",
    "    # Let's adjust the empirical data for a more direct comparison.\n",
    "    results['empirical_y'] = probabilities # Keep as PMF for clarity\n",
    "    results['fit_y'] = 10**(slope * np.log10(results['empirical_x']) + intercept) * avg_bin_width # Re-evaluate on empirical x\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def crit_binarize_copy(A, thresh=0.0):\n",
    "    # Hard copy so nothing downstream can alias the caller's buffer\n",
    "    Af = np.array(A, dtype=float, copy=True)\n",
    "    Af = np.nan_to_num(Af, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return (Af > thresh).astype(np.uint8, copy=False)  # internal, never returned\n",
    "\n",
    "def _label_components_8c(mask):\n",
    "    # Strictly read-only w.r.t. input\n",
    "    try:\n",
    "        from scipy.ndimage import label\n",
    "        structure = np.ones((3,3), dtype=np.uint8)\n",
    "        labels, nlab = label(mask, structure=structure)\n",
    "        return labels.astype(np.int32, copy=False), int(nlab)\n",
    "    except Exception:\n",
    "        H, W = mask.shape\n",
    "        labels = np.zeros_like(mask, dtype=np.int32)\n",
    "        nlab = 0\n",
    "        nbrs = [(di,dj) for di in (-1,0,1) for dj in (-1,0,1) if (di or dj)]\n",
    "        for i in range(H):\n",
    "            js = np.where(mask[i] & (labels[i]==0))[0]\n",
    "            for j in js:\n",
    "                if labels[i,j] != 0: continue\n",
    "                nlab += 1\n",
    "                stack = [(i,j)]\n",
    "                labels[i,j] = nlab\n",
    "                while stack:\n",
    "                    ti,tj = stack.pop()\n",
    "                    for di,dj in nbrs:\n",
    "                        ui = ti + di\n",
    "                        uj = (tj + dj) % W  # periodic in space, no wrap in time\n",
    "                        if ui < 0 or ui >= H: continue\n",
    "                        if mask[ui,uj] and labels[ui,uj]==0:\n",
    "                            labels[ui,uj] = nlab\n",
    "                            stack.append((ui,uj))\n",
    "        return labels, nlab\n",
    "\n",
    "def _per_label_area(lbl_img, num_labels):\n",
    "    if num_labels == 0: return np.zeros(0, np.int64)\n",
    "    bc = np.bincount(lbl_img.ravel(), minlength=num_labels+1)\n",
    "    return bc[1:].astype(np.int64)\n",
    "\n",
    "def _per_label_unique(lbls, coords):\n",
    "    if lbls.size == 0: return np.zeros(0, np.int64)\n",
    "    pairs = np.stack([lbls, coords], axis=1)\n",
    "    uniq = np.unique(pairs, axis=0)\n",
    "    return np.bincount(uniq[:,0], minlength=int(uniq[:,0].max())+1)[1:]\n",
    "\n",
    "def _first10_hist(values):\n",
    "    if len(values)==0:\n",
    "        counts = np.zeros(10, np.int64)\n",
    "    else:\n",
    "        v = np.asarray(values, np.int64)\n",
    "        v = v[v>=1]\n",
    "        v = np.minimum(v, 10)            # clip ≥10 into bin 10\n",
    "        counts = np.bincount(v, minlength=11)[1:11]\n",
    "    nonzero = (counts>0).sum()\n",
    "    Bpct = nonzero/10.0\n",
    "    return counts.astype(np.float64), Bpct\n",
    "\n",
    "def _fit_ls_R2_KS(counts10):\n",
    "    k = np.arange(1,11, dtype=np.float64)\n",
    "    y = counts10.astype(np.float64)\n",
    "    nz = y>0\n",
    "    if nz.sum() <= 1:\n",
    "        return 0.0, 1.0\n",
    "    p = y[nz] / y[nz].sum()\n",
    "    x = np.log10(k[nz])\n",
    "    t = np.log10(p)\n",
    "    A = np.vstack([x, np.ones_like(x)]).T\n",
    "    b, a = np.linalg.lstsq(A, t, rcond=None)[0]  # log10 p ≈ a + b log10 k\n",
    "    t_hat = a + b*x\n",
    "    ss_res = np.sum((t - t_hat)**2)\n",
    "    ss_tot = np.sum((t - t.mean())**2)\n",
    "    R2 = 1.0 - (ss_res/ss_tot if ss_tot>0 else 1.0)\n",
    "    alpha = -b\n",
    "    w = (k[nz] ** (-alpha))\n",
    "    w /= w.sum()\n",
    "    D = float(np.max(np.abs(np.cumsum(p) - np.cumsum(w))))\n",
    "    return float(np.clip(R2, -1.0, 1.0)), float(np.clip(D, 0.0, 1.0))\n",
    "\n",
    "def _aggregate_scores(R2s, Ds, Bs, U):\n",
    "    # Eqns (2)-(7) with the paper’s α; we map small KS→high score with a minus sign in exp\n",
    "    aR2, aD, aB, aL = 0.01, 1.0, 5.0, 0.01\n",
    "\n",
    "    R2s = np.asarray(R2s); Ds = np.asarray(Ds); Bs = np.asarray(Bs)\n",
    "    R2_0, R2_1, R2_all = R2s[:3].mean(), R2s[3:].mean(), R2s.mean()\n",
    "    D_0,  D_1,  D_all  = Ds[:3].mean(),  Ds[3:].mean(),  Ds.mean()\n",
    "    B_0,  B_1,  B_all  = Bs[:3].mean(),  Bs[3:].mean(),  Bs.mean()\n",
    "\n",
    "    sig = lambda z: 1.0/(1.0+np.exp(-z))\n",
    "    Rhat = sig(aR2*(0.9*max(R2_0,R2_1)+0.1*R2_all))                       # (2)\n",
    "    Dhat = float(np.exp(-aD*(0.9*min(D_0,D_1)+0.1*D_all)))                # (3) mapped so smaller KS→larger score\n",
    "    Bhat = float(np.tanh(aB*(0.9*max(B_0,B_1)+0.1*B_all)))                # (4)\n",
    "\n",
    "    Spartial = (Rhat**2) + (Dhat**2) + Bhat + float(U)                    # (6)\n",
    "    return float(Spartial)                                                # (7) w/o L̂ for speed/robustness\n",
    "\n",
    "\n"
   ],
   "id": "f54d9fba35743a86",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:48:30.831881Z",
     "start_time": "2025-09-30T20:48:30.818284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CEM1Layer:\n",
    "\n",
    "    def __init__(self, N, T, dt, alpha_m,\n",
    "                 theta=None, gamma=0.0, rng_seed=42, init_scale=1.0):\n",
    "        self.N = int(N)\n",
    "        self.T = int(T)\n",
    "        self.dt = float(dt)\n",
    "        self.alpha_m = float(alpha_m)   # kept for interface compatibility\n",
    "        self.gamma = float(gamma)\n",
    "\n",
    "\n",
    "\n",
    "        if theta is None:\n",
    "            self.theta = {\n",
    "                \"k0\": random.uniform(-0.10,  0.10),\n",
    "                \"k1\": random.uniform(-0.40,  0.40),\n",
    "                \"k2\": random.uniform(-0.60,  0.60),\n",
    "                \"k3\": random.uniform(-0.40,  0.40),\n",
    "                \"k4\": random.uniform(-0.30,  0.30),\n",
    "                \"k5\": random.uniform(-0.30,  0.30),\n",
    "                \"k6\": random.uniform(-0.15,  0.15),\n",
    "            }\n",
    "        else:\n",
    "            keys = [\"k0\",\"k1\",\"k2\",\"k3\",\"k4\",\"k5\",\"k6\"]\n",
    "            self.theta = {k: float(theta.get(k, 0.0)) for k in keys}\n",
    "\n",
    "        # Allocate state history buffers to mirror your class\n",
    "        self.A = np.empty((self.T + 1, self.N), dtype=float)\n",
    "\n",
    "\n",
    "        # Fitness bookkeeping (mirrors your attributes)\n",
    "        self.fitness = -1e3\n",
    "        self.fit_res = None\n",
    "\n",
    "        # Initialize state (A[0]) like your __init_state__\n",
    "        self.__init_state__(state=None, rng_seed=rng_seed, init_scale=init_scale)\n",
    "\n",
    "    # ---- Initialization (mirrors your signature/behavior) ----\n",
    "    def __init_state__(self, state=None, rng_seed=42, init_scale=1.0):\n",
    "        \"\"\"\n",
    "        If state is None: random normal initial A[0].\n",
    "        If state is a tuple/list (A0, M0) from old 2-layer code, we use only A0.\n",
    "        If state is a 1D array of shape (N,), we use it as A[0].\n",
    "        \"\"\"\n",
    "        if state is None:\n",
    "            rng = np.random.default_rng(rng_seed)\n",
    "            a0 = init_scale * rng.standard_normal(self.N)\n",
    "            self.A[0] = a0\n",
    "            return self.A\n",
    "\n",
    "        else:\n",
    "            a0 = np.asarray(state, dtype=float)\n",
    "\n",
    "        assert a0.shape == (self.N,), f\"Initial state must be shape (N,), got {a0.shape}\"\n",
    "        self.A[0] = a0\n",
    "        return self.A\n",
    "\n",
    "    # ---- Dynamics (periodic boundary; RK4 integrator) ----\n",
    "    def deriv(self, a):\n",
    "        \"\"\"Compute ds/dt for the current state a (periodic neighborhood).\"\"\"\n",
    "        l = np.roll(a,  1)\n",
    "        r = np.roll(a, -1)\n",
    "        c = a\n",
    "        t = self.theta\n",
    "        dot = (t[\"k0\"]*0.0\n",
    "               + t[\"k1\"]*l + t[\"k2\"]*c + t[\"k3\"]*r\n",
    "               + t[\"k4\"]*(c*r) + t[\"k5\"]*(c*l)\n",
    "               + t[\"k6\"]*(l*c*r)\n",
    "               - self.gamma * c)\n",
    "        return dot\n",
    "\n",
    "    def rk4_step(self, a):\n",
    "        \"\"\"One RK4 step for ds/dt = f(a).\"\"\"\n",
    "        k1 = self.deriv(a)\n",
    "        k2 = self.deriv(a + 0.5*self.dt*k1)\n",
    "        k3 = self.deriv(a + 0.5*self.dt*k2)\n",
    "        k4 = self.deriv(a + self.dt*k3)\n",
    "        a_next = a + (self.dt/6.0)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "        return a_next\n",
    "\n",
    "\n",
    "    def simulate(self):\n",
    "        \"\"\"Run T integration steps, filling self.A[1:].\"\"\"\n",
    "        for t in range(1, self.T + 1):\n",
    "            a_prev = self.A[t - 1]\n",
    "            a_next = self.rk4_step(a_prev)\n",
    "            self.A[t] = a_next\n",
    "\n",
    "    # ---- Helpers to mirror ergonomics ----\n",
    "    def set_theta(self, **kwargs):\n",
    "        \"\"\"Update any subset of k0..k6 (e.g., set_theta(k2=0.1, k6=-0.05)).\"\"\"\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in self.theta:\n",
    "                raise KeyError(f\"Unknown coefficient '{k}'. Valid keys: {list(self.theta.keys())}\")\n",
    "            self.theta[k] = float(v)\n",
    "\n",
    "    def get_theta(self):\n",
    "        return dict(self.theta)\n",
    "\n",
    "    def compute_criticality_from_paper(self,A, return_plot_data=False):\n",
    "        \"\"\"\n",
    "        Computes the criticality fitness score as described by Pontes-Filho et al.\n",
    "        Optionally returns data required for plotting the six avalanche distributions.\n",
    "        \"\"\"\n",
    "        # Threshold A at 0 to get a binary matrix\n",
    "        binarized_A = (A > 0).astype(int)\n",
    "\n",
    "        # 1. Get avalanche distributions (same as before)\n",
    "        sizes0, durations0, areas0 = _get_avalanches(binarized_A, state=0)\n",
    "        sizes1, durations1, areas1 = _get_avalanches(binarized_A, state=1)\n",
    "        all_distributions = [sizes0, durations0, areas0, sizes1, durations1, areas1]\n",
    "\n",
    "        if any(len(dist) == 0 for dist in all_distributions):\n",
    "            return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "        # 2. Calculate metrics and gather plot data\n",
    "        R2_values = []\n",
    "        B_values = []\n",
    "        plot_data_list = []\n",
    "\n",
    "        for dist in all_distributions:\n",
    "            metrics_and_plot_data = _calculate_metrics_from_dist(dist)\n",
    "            R2_values.append(metrics_and_plot_data['R2'])\n",
    "            B_values.append(metrics_and_plot_data['B'])\n",
    "            plot_data_list.append(metrics_and_plot_data)\n",
    "\n",
    "        R2_values = np.nan_to_num(R2_values)\n",
    "        B_values = np.nan_to_num(B_values)\n",
    "\n",
    "        # 3. Combine scores (same logic as before)\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        alpha_R2, alpha_B = 0.01, 5.0\n",
    "\n",
    "        R2_0_avg, R2_1_avg = np.mean(R2_values[:3]), np.mean(R2_values[3:])\n",
    "        B_0_avg, B_1_avg = np.mean(B_values[:3]), np.mean(B_values[3:])\n",
    "\n",
    "        R2_score = sigmoid(alpha_R2 * (0.9 * max(R2_0_avg, R2_1_avg) + 0.1 * np.mean(R2_values)))\n",
    "        B_score = np.tanh(alpha_B * (0.9 * max(B_0_avg, B_1_avg) + 0.1 * np.mean(B_values)))\n",
    "\n",
    "        mean_state = np.mean(binarized_A)\n",
    "        U_score = 1.0 - np.abs(mean_state - 0.5) * 2\n",
    "\n",
    "        S_final = (R2_score)**2 + B_score + U_score\n",
    "\n",
    "        # 4. Return the results\n",
    "        if return_plot_data:\n",
    "            return S_final, plot_data_list\n",
    "        else:\n",
    "            self.fit_res = plot_data_list\n",
    "            return S_final\n",
    "\n",
    "    def criticality_fitness_pure(self,A, thresh=0.0):\n",
    "        \"\"\"\n",
    "        Pure, read-only scorer. Computes a single scalar S to maximize.\n",
    "        Never mutates the caller's array; never imports 'powerlaw'.\n",
    "        \"\"\"\n",
    "        # 1) Binarize on a COPY\n",
    "        Ab = crit_binarize_copy(A, thresh=thresh)\n",
    "        T1, N = Ab.shape\n",
    "\n",
    "        # 2) U: percent of unique global states over time\n",
    "        U = np.unique(Ab, axis=0).shape[0] / float(T1)\n",
    "\n",
    "        R2s, Ds, Bs = [], [], []\n",
    "\n",
    "        # 3) For each state (0/1): label 3x3 space-time avalanches; build size/duration/area dists\n",
    "        for state in (0,1):\n",
    "            mask = (Ab==state)\n",
    "            labels, nlab = _label_components_8c(mask)\n",
    "            if nlab==0:\n",
    "                sizes = durs = areas = np.array([], np.int64)\n",
    "            else:\n",
    "                tt, xx = np.nonzero(labels)\n",
    "                labs = labels[tt,xx]\n",
    "                areas = _per_label_area(labels, nlab)\n",
    "                durs  = _per_label_unique(labs, tt)\n",
    "                sizes = _per_label_unique(labs, xx)\n",
    "\n",
    "            for meas in (sizes, durs, areas):\n",
    "                counts10, Bpct = _first10_hist(meas)\n",
    "                if (counts10>0).sum() <= 5:\n",
    "                    return 0.0                 # gating rule from the paper\n",
    "                R2, D = _fit_ls_R2_KS(counts10)\n",
    "                R2s.append(R2); Ds.append(D); Bs.append(Bpct)\n",
    "\n",
    "        # 4) Scalarize (Eqns 2–7, without L̂)\n",
    "        return _aggregate_scores(R2s, Ds, Bs, U)\n",
    "\n",
    "    def compute_criticality_metric_numpy(self,A,\n",
    "                                     threshold_z_score=1.5,\n",
    "                                     target_alpha=1.5,\n",
    "                                     sigma_alpha=0.5,\n",
    "                                     return_plot_data=False):\n",
    "        \"\"\"\n",
    "        Computes a criticality metric using a pure NumPy/SciPy power-law fit.\n",
    "        Optionally returns data required for plotting the distribution and the fit.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            activity_mean = np.mean(A)\n",
    "            activity_std = np.std(A)\n",
    "            if activity_std == 0:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            threshold = activity_mean + threshold_z_score * activity_std\n",
    "            binarized_A = A > 0\n",
    "\n",
    "            labeled_array, num_avalanches = label(binarized_A)\n",
    "            if num_avalanches < 2:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            avalanche_sizes = np.bincount(labeled_array.ravel())[1:]\n",
    "\n",
    "            if len(avalanche_sizes) < 30:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            # Call our fitter, which now returns a dictionary\n",
    "            fit_results = fit_power_law_numpy(avalanche_sizes)\n",
    "            self.fit_res = fit_results\n",
    "            if not fit_results['fit_successful']:\n",
    "                return 0.0 if not return_plot_data else (0.0, fit_results)\n",
    "\n",
    "            # Calculate fitness from the results\n",
    "            goodness_of_fit_score = 1.0 - fit_results['ks_distance']\n",
    "            slope_score = np.exp(-((fit_results['alpha'] - target_alpha)**2) / (2 * sigma_alpha**2))\n",
    "            final_fitness = max(0, goodness_of_fit_score * slope_score)\n",
    "\n",
    "            # Return either just the fitness or the fitness and the plot data\n",
    "            if return_plot_data:\n",
    "                return final_fitness, fit_results\n",
    "            else:\n",
    "                return final_fitness\n",
    "\n",
    "        except Exception:\n",
    "            return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "\n",
    "    def cem_criticality_fitness_port(self,A_continuous):\n",
    "        \"\"\"\n",
    "        A_continuous: np.ndarray of shape (T+1, N) or (T, N), real-valued from CEM.\n",
    "        Returns positive fitness (maximize this).\n",
    "        This is a literal port of the paper’s evaluator, operating on A>0 binarization.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A_continuous)\n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(f\"A must be 2D (T x N), got shape {A.shape}\")\n",
    "        # If you pass (T+1,N), that's fine — the paper just treats rows as timesteps.\n",
    "        Abin = (A > 0).astype(np.uint8)\n",
    "        fitness, _ = evaluate_result_port(Abin)\n",
    "        return float(fitness)\n",
    "\n",
    "    def compute_fitness(self, bound=40.0, penalty_strength=2.0):\n",
    "        \"\"\"\n",
    "        Runs simulate(), scores with criticality_fitness(), and applies a soft\n",
    "        penalty if any cells in A leave [-bound, bound]. Robust to NaNs/Infs.\n",
    "        \"\"\"\n",
    "        np.seterr(invalid=\"ignore\", divide=\"ignore\", over=\"ignore\", under=\"ignore\")\n",
    "        self.simulate()\n",
    "\n",
    "        A = self.A\n",
    "        # If the sim blew up, assign zero fitness.\n",
    "        if not np.all(np.isfinite(A)):\n",
    "            self.fitness = -10000\n",
    "            return self.fitness\n",
    "\n",
    "        # Base score (guard NaNs)\n",
    "        base = self.compute_criticality_metric_numpy(A)\n",
    "        base2= self.cem_criticality_fitness_port(A)\n",
    "        base += base2\n",
    "        if not np.isfinite(base):\n",
    "            base = -10000.0\n",
    "\n",
    "        # Soft violation metric: how far |A| exceeds 'bound'\n",
    "        # excess[i,t] = max(0, |A| - bound)\n",
    "        excess = np.maximum(0.0, np.abs(A) - float(bound))\n",
    "        if excess.size == 0:\n",
    "            self.fitness = float(base)\n",
    "            return self.fitness\n",
    "\n",
    "        v_mean = float(np.mean(excess))\n",
    "        v_max  = float(np.max(excess))\n",
    "        # Combine mean + max so brief spikes are punished too\n",
    "        v = 0.7 * v_mean + 0.3 * v_max\n",
    "\n",
    "        # Exponential penalty factor in (0,1]; stronger with larger 'penalty_strength'\n",
    "        penalty = float(np.exp(-penalty_strength * v))\n",
    "\n",
    "        # Optional hard fail if it really blows past the bound (comment out if undesired)\n",
    "        if v_max > bound:  # e.g., any cell beyond ±(bound+bound) ⇒ zero\n",
    "            penalty = 0.0\n",
    "\n",
    "        self.fitness = float(base * penalty)\n",
    "        return self.fitness"
   ],
   "id": "515be4964f60bd0c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:52:42.149979Z",
     "start_time": "2025-09-30T20:48:30.876577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cma\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# from your_module import CEM1Layer\n",
    "\n",
    "class Objective:  # top-level, picklable\n",
    "    def __init__(self, N, T, dt, alpha_m, gamma, theta_keys):\n",
    "        self.N = N; self.T = T; self.dt = dt\n",
    "        self.alpha_m = alpha_m; self.gamma = gamma\n",
    "        self.theta_keys = theta_keys\n",
    "\n",
    "    def __call__(self, theta_vector):\n",
    "        theta = dict(zip(self.theta_keys, theta_vector))\n",
    "        cem = CEM1Layer(N=self.N, T=self.T, dt=self.dt,\n",
    "                        alpha_m=self.alpha_m, gamma=self.gamma, theta=theta, init_scale=0.1)\n",
    "        return -cem.compute_fitness()\n",
    "\n",
    "def evolve_cem_with_cma(\n",
    "    N, T, dt, alpha_m, gamma=0.0,\n",
    "    n_generations=50,\n",
    "    population_size=12,\n",
    "    sigma0=0.5,\n",
    "    n_workers=None\n",
    "):\n",
    "    theta_keys = [\"k0\", \"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\"]\n",
    "    x0 = np.zeros(len(theta_keys))\n",
    "    es = cma.CMAEvolutionStrategy(x0, sigma0, {'popsize': population_size})\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = min(population_size, max(1, os.cpu_count() or 1))\n",
    "\n",
    "    obj = Objective(N, T, dt, alpha_m, gamma, theta_keys)\n",
    "\n",
    "    print(f\"Starting CMA-ES for {n_generations} generations with {n_workers} workers (joblib/loky)...\")\n",
    "\n",
    "    for gen in range(n_generations):\n",
    "        X = es.ask()\n",
    "        # loky backend = separate processes; cloudpickle handles __main__/closures\n",
    "        fvals = Parallel(n_jobs=n_workers)(\n",
    "            delayed(obj)(x) for x in X\n",
    "        )\n",
    "        es.tell(X, fvals)\n",
    "        es.disp()\n",
    "        print(f\"Generation {gen + 1}/{n_generations} | Best Fitness = {-es.result.fbest:.4f}\")\n",
    "\n",
    "    best_vec = es.result.xbest\n",
    "    best_theta = dict(zip(theta_keys, best_vec))\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    for k, v in best_theta.items():\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "\n",
    "    fittest = CEM1Layer(N=N, T=T, dt=dt, alpha_m=alpha_m, gamma=gamma, theta=best_theta)\n",
    "    fittest.compute_fitness()\n",
    "    return fittest\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EXAMPLE USAGE\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Define the parameters for our CA and the evolution ---\n",
    "\n",
    "    # CA Parameters\n",
    "    N_CELLS = 1000\n",
    "    T_STEPS = 1000\n",
    "    DT = 0.3\n",
    "    ALPHA_M = 0.1 # Not used in this version but kept for compatibility\n",
    "    GAMMA = 0.00    # A small decay term\n",
    "\n",
    "    # Evolution Parameters\n",
    "    GENERATIONS = 20 # Use a smaller number for a quick test\n",
    "    POP_SIZE = 96   # CMA-ES works well with relatively small populations\n",
    "    INITIAL_SIGMA = 1# A good starting search radius for params ~[-1, 1]\n",
    "\n",
    "    # --- Run the evolution ---\n",
    "    best_individual = evolve_cem_with_cma(\n",
    "        N=N_CELLS, T=T_STEPS, dt=DT, alpha_m=ALPHA_M, gamma=GAMMA,\n",
    "        n_generations=GENERATIONS,\n",
    "        population_size=POP_SIZE,\n",
    "        sigma0=INITIAL_SIGMA,\n",
    "        n_workers=20\n",
    "    )\n",
    "\n",
    "    # --- Analyze the result ---\n",
    "    print(\"\\n--- Analysis of the Fittest Individual ---\")\n",
    "    print(f\"Final Fitness Score: {best_individual.fitness:.4f}\")\n",
    "\n",
    "    # You can now, for example, get the plotting data from this best individual\n",
    "    # Note: The data is already stored in best_individual.fit_res from the last\n",
    "    # compute_fitness() call inside the evolution function.\n",
    "    plot_data = best_individual.fit_res\n",
    "\n",
    "    # You could now pass this 'plot_data' to your plotting function\n",
    "    # to visualize the criticality of the final evolved CA.\n",
    "    # For example:\n",
    "    # plot_paper_criticiality_fits(plot_data)\n",
    "\n",
    "    print(\"\\nPlotting data for the 6 distributions is available in `best_individual.fit_res`.\")"
   ],
   "id": "535b18cd576449d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48_w,96)-aCMA-ES (mu_w=25.9,w_1=8%) in dimension 7 (seed=393618, Tue Sep 30 22:48:30 2025)\n",
      "Starting CMA-ES for 20 generations with 20 workers (joblib/loky)...\n",
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1     96 -2.466209385363102e+00 1.0e+00 1.21e+00  1e+00  1e+00 0:03.5\n",
      "Generation 1/20 | Best Fitness = 2.4662\n",
      "    2    192 -2.615929968296593e+00 1.5e+00 1.41e+00  1e+00  2e+00 0:08.8\n",
      "Generation 2/20 | Best Fitness = 2.6159\n",
      "    3    288 -2.390541094897992e+00 1.9e+00 1.69e+00  2e+00  2e+00 0:14.6\n",
      "Generation 3/20 | Best Fitness = 2.6159\n",
      "    4    384 -2.533371907795476e+00 2.6e+00 1.79e+00  1e+00  2e+00 0:21.2\n",
      "Generation 4/20 | Best Fitness = 2.6159\n",
      "    5    480 -2.474805470903390e+00 3.2e+00 1.99e+00  1e+00  2e+00 0:27.1\n",
      "Generation 5/20 | Best Fitness = 2.6159\n",
      "    6    576 -2.523330971642086e+00 4.2e+00 1.87e+00  1e+00  2e+00 0:34.3\n",
      "Generation 6/20 | Best Fitness = 2.6159\n",
      "    7    672 -2.793479264698571e+00 5.1e+00 1.70e+00  1e+00  2e+00 0:44.0\n",
      "Generation 7/20 | Best Fitness = 2.7935\n",
      "Generation 8/20 | Best Fitness = 2.7935\n",
      "    9    864 -2.821097677992107e+00 6.6e+00 1.57e+00  7e-01  1e+00 1:01.2\n",
      "Generation 9/20 | Best Fitness = 2.8211\n",
      "Generation 10/20 | Best Fitness = 2.8211\n",
      "   11   1056 -2.528988677108936e+00 8.7e+00 1.83e+00  7e-01  2e+00 1:14.3\n",
      "Generation 11/20 | Best Fitness = 2.8211\n",
      "Generation 12/20 | Best Fitness = 2.8211\n",
      "   13   1248 -2.470599874158191e+00 8.2e+00 1.93e+00  6e-01  2e+00 1:25.6\n",
      "Generation 13/20 | Best Fitness = 2.8211\n",
      "Generation 14/20 | Best Fitness = 2.8211\n",
      "   15   1440 -2.449618192860624e+00 1.2e+01 2.30e+00  7e-01  3e+00 1:36.9\n",
      "Generation 15/20 | Best Fitness = 2.8211\n",
      "Generation 16/20 | Best Fitness = 2.9755\n",
      "   17   1632 -2.472542440880696e+00 1.5e+01 2.62e+00  7e-01  3e+00 2:01.0\n",
      "Generation 17/20 | Best Fitness = 2.9755\n",
      "   18   1728 -2.531434779682655e+00 1.6e+01 3.00e+00  8e-01  3e+00 2:51.9\n",
      "Generation 18/20 | Best Fitness = 2.9755\n",
      "   19   1824 -2.498313650148585e+00 2.0e+01 2.98e+00  7e-01  3e+00 4:04.4\n",
      "Generation 19/20 | Best Fitness = 2.9755\n",
      "Generation 20/20 | Best Fitness = 2.9755\n",
      "\n",
      "Best parameters found:\n",
      "  k0: 5.223727\n",
      "  k1: 1.518652\n",
      "  k2: -0.717248\n",
      "  k3: -1.496489\n",
      "  k4: -1.110468\n",
      "  k5: -1.049467\n",
      "  k6: -4.126583\n",
      "\n",
      "--- Analysis of the Fittest Individual ---\n",
      "Final Fitness Score: -10000.0000\n",
      "\n",
      "Plotting data for the 6 distributions is available in `best_individual.fit_res`.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:53:21.334971Z",
     "start_time": "2025-09-30T20:53:21.294943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_paper_criticiality_fits(plot_data_list):\n",
    "    \"\"\"\n",
    "    Generates a 2x3 grid of log-log plots to replicate Figure 4 from the paper,\n",
    "    using the data returned by compute_criticality_from_paper.\n",
    "\n",
    "    Args:\n",
    "        plot_data_list (list): A list of 6 dictionaries, one for each distribution.\n",
    "    \"\"\"\n",
    "    if plot_data_list is None or len(plot_data_list) != 6:\n",
    "        print(\"Invalid plot data provided. Cannot generate plots.\")\n",
    "        return\n",
    "\n",
    "    titles = [\n",
    "        'Avalanche size of state 0', 'Avalanche duration of state 0', 'Avalanche area of state 0',\n",
    "        'Avalanche size of state 1', 'Avalanche duration of state 1', 'Avalanche area of state 1'\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        data = plot_data_list[i]\n",
    "\n",
    "        if data['empirical_x'] is None:\n",
    "            ax.set_title(f\"{titles[i]}\\n(No data)\")\n",
    "            continue\n",
    "\n",
    "        # Plot the empirical data (blue line in the paper)\n",
    "        ax.plot(data['empirical_x'], data['empirical_y'], 'b-', marker='o', markersize=4,\n",
    "                label='Empirical Data (PDF)')\n",
    "\n",
    "        # Plot the fitted line (dashed black line in the paper)\n",
    "        if data['fit_x'] is not None:\n",
    "            ax.plot(data['fit_x'], data['fit_y'], 'k--', linewidth=2,\n",
    "                    label=f'Fit (α={data[\"alpha\"]:.2f})')\n",
    "\n",
    "        # --- Formatting ---\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('P(x)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "a = best_individual.A\n",
    "print(a.min(), a.max())\n",
    "plt.imshow(a[:100,:100]<0, cmap=\"gray\")"
   ],
   "id": "e0e3f9b8dc9fefe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x75c0237de870>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:52:42.215274Z",
     "start_time": "2025-09-30T20:52:42.213991Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4d1d555246da30e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:52:42.259661Z",
     "start_time": "2025-09-30T20:52:42.257992Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d5d029955cc6cb54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T20:52:42.304135Z",
     "start_time": "2025-09-30T20:52:42.302377Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "21b27ec97560db31",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
