{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:05:09.632587Z",
     "start_time": "2025-09-30T21:05:09.239055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from scipy.stats import linregress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.ndimage import label\n",
    "import powerlaw"
   ],
   "id": "1fda7c63c735c838",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:05:09.643294Z",
     "start_time": "2025-09-30T21:05:09.636105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.special import zeta # For normalization\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage import find_objects\n",
    "\n",
    "def fit_power_law_numpy(data):\n",
    "    \"\"\"\n",
    "    Fits a power-law distribution to discrete data using the Clauset, Shalizi, Newman method.\n",
    "\n",
    "    Returns a dictionary containing fit parameters and data for plotting.\n",
    "    \"\"\"\n",
    "    # --- The first part of the function (finding the best alpha and xmin) is the same ---\n",
    "    possible_xmins = np.unique(data)\n",
    "    possible_xmins = possible_xmins[possible_xmins < np.max(possible_xmins)]\n",
    "\n",
    "    if len(possible_xmins) < 2:\n",
    "        return {'alpha': 1.0, 'xmin': 1, 'ks_distance': 1.0, 'fit_successful': False}\n",
    "\n",
    "    ks_distances = []\n",
    "    alphas = []\n",
    "\n",
    "    for xmin_candidate in possible_xmins:\n",
    "        tail_data = data[data >= xmin_candidate]\n",
    "        n_tail = len(tail_data)\n",
    "        if n_tail < 10:\n",
    "            ks_distances.append(np.inf)\n",
    "            alphas.append(1.0)\n",
    "            continue\n",
    "\n",
    "        alpha_mle = 1.0 + n_tail / np.sum(np.log(tail_data / (xmin_candidate - 0.5)))\n",
    "        alphas.append(alpha_mle)\n",
    "\n",
    "        empirical_cdf = np.arange(1, n_tail + 1) / n_tail\n",
    "        unique_tail_data, counts = np.unique(tail_data, return_counts=True)\n",
    "\n",
    "        # This part has been simplified for clarity, the logic is the same\n",
    "        theoretical_cdf = np.cumsum((unique_tail_data**-alpha_mle) / zeta(alpha_mle, xmin_candidate))\n",
    "        cdf_map = dict(zip(unique_tail_data, theoretical_cdf))\n",
    "        theoretical_cdf_full = np.array([cdf_map[val] for val in tail_data])\n",
    "\n",
    "        ks_distance = np.max(np.abs(empirical_cdf - theoretical_cdf_full))\n",
    "        ks_distances.append(ks_distance)\n",
    "\n",
    "    if np.all(np.isinf(ks_distances)):\n",
    "        return {'alpha': 1.0, 'xmin': 1, 'ks_distance': 1.0, 'fit_successful': False}\n",
    "\n",
    "    best_idx = np.argmin(ks_distances)\n",
    "    best_xmin = possible_xmins[best_idx]\n",
    "    best_alpha = alphas[best_idx]\n",
    "    min_ks_distance = ks_distances[best_idx]\n",
    "\n",
    "    # --- NEW: Calculate data for plotting ---\n",
    "\n",
    "    # 1. Empirical PDF (Probability Density Function)\n",
    "    unique_sizes, counts = np.unique(data, return_counts=True)\n",
    "    empirical_pdf_y = counts / len(data)\n",
    "    empirical_pdf_x = unique_sizes\n",
    "\n",
    "    # 2. Theoretical PDF (the fitted line)\n",
    "    # The line should only be drawn for x >= xmin\n",
    "    fit_pdf_x = np.arange(best_xmin, np.max(data) + 1, dtype=np.float64)\n",
    "    # Normalization constant C = 1 / zeta(alpha, xmin)\n",
    "    normalization_const = 1 / zeta(best_alpha, best_xmin)\n",
    "    fit_pdf_y = normalization_const * (fit_pdf_x ** -best_alpha)\n",
    "\n",
    "    # 3. Package everything into a dictionary\n",
    "    results = {\n",
    "        'alpha': best_alpha,\n",
    "        'xmin': best_xmin,\n",
    "        'ks_distance': min_ks_distance,\n",
    "        'fit_successful': True,\n",
    "        'empirical_pdf_x': empirical_pdf_x,\n",
    "        'empirical_pdf_y': empirical_pdf_y,\n",
    "        'fit_pdf_x': fit_pdf_x,\n",
    "        'fit_pdf_y': fit_pdf_y\n",
    "    }\n",
    "\n",
    "    return results\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "def _get_avalanches(matrix, state):\n",
    "    \"\"\"\n",
    "    [Corrected Version - Reflects Figure 1's definitions]\n",
    "    Finds all spatiotemporal avalanches and calculates their size, duration, and area\n",
    "    according to the specific definitions in the Pontes-Filho et al. paper.\n",
    "\n",
    "    - size: Number of unique spatial cells (columns) affected.\n",
    "    - duration: Number of time-steps the avalanche lasts.\n",
    "    - area: Total number of spatiotemporal cells in the cluster.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): The binarized (T, N) state matrix.\n",
    "        state (int): The state to search for (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists: (sizes, durations, areas).\n",
    "    \"\"\"\n",
    "    T, N = matrix.shape\n",
    "    state_matrix = (matrix == state)\n",
    "\n",
    "    structure = np.ones((3, 3), dtype=int)\n",
    "    labeled_matrix, num_avalanches = label(state_matrix, structure=structure)\n",
    "\n",
    "    if num_avalanches == 0:\n",
    "        return [], [], []\n",
    "\n",
    "    # --- Area Calculation (total cells in cluster) ---\n",
    "    # This part was already correct.\n",
    "    # np.bincount is the most efficient way to get the size of each labeled region.\n",
    "    # We discard the count for label 0, which is the background.\n",
    "    areas = np.bincount(labeled_matrix.ravel())[1:]\n",
    "\n",
    "    # --- Duration Calculation (temporal extent) ---\n",
    "    # This part was also correct.\n",
    "    # find_objects gives us the bounding box for each labeled avalanche.\n",
    "    locations = find_objects(labeled_matrix)\n",
    "    durations = [loc[0].stop - loc[0].start for loc in locations]\n",
    "\n",
    "    # --- Size Calculation (unique spatial cells affected) ---\n",
    "    # This is the new, corrected logic.\n",
    "\n",
    "    # Create an array where each element is its own column index.\n",
    "    # e.g., for N=4, it's [[0,1,2,3], [0,1,2,3], ...]\n",
    "    cols_grid = np.tile(np.arange(N), (T, 1))\n",
    "\n",
    "    # Flatten both the labels and the column indices.\n",
    "    labels_flat = labeled_matrix.ravel()\n",
    "    cols_flat = cols_grid.ravel()\n",
    "\n",
    "    # We only care about cells that are part of an avalanche (label > 0).\n",
    "    mask = labels_flat > 0\n",
    "    active_labels = labels_flat[mask]\n",
    "    active_cols = cols_flat[mask]\n",
    "\n",
    "    # Create a 2D array of [label, column] pairs.\n",
    "    label_col_pairs = np.stack([active_labels, active_cols], axis=1)\n",
    "\n",
    "    # Find the unique pairs. This is the key step.\n",
    "    # For each avalanche, this effectively lists each column it touches exactly once.\n",
    "    unique_label_col_pairs = np.unique(label_col_pairs, axis=0)\n",
    "\n",
    "    # Now, count how many unique columns belong to each label.\n",
    "    # The first column of unique_label_col_pairs contains the labels.\n",
    "    # By counting the occurrences of each label ID, we get the 'size'.\n",
    "    unique_labels, sizes = np.unique(unique_label_col_pairs[:, 0], return_counts=True)\n",
    "\n",
    "    # We need to ensure the sizes array is correctly ordered and sized in case some\n",
    "    # label IDs were skipped (though this is unlikely with np.unique).\n",
    "    # We create a zero-filled array and place the counts at the correct index.\n",
    "    final_sizes = np.zeros(num_avalanches, dtype=int)\n",
    "    # unique_labels are 1-based, so we subtract 1 for 0-based indexing.\n",
    "    final_sizes[unique_labels.astype(int) - 1] = sizes\n",
    "\n",
    "    return final_sizes.tolist(), durations, areas.tolist()\n",
    "def _calculate_metrics_from_dist(distribution, num_bins=10):\n",
    "    \"\"\"\n",
    "    [Corrected Version 4]\n",
    "    Calculates R^2 and alpha by fitting a line to the log-log plot of the\n",
    "    PROBABILITY DENSITY, which correctly handles logarithmic binning.\n",
    "    This will produce the correct positive alpha.\n",
    "    \"\"\"\n",
    "    total_avalanches = len(distribution)\n",
    "    results = {\n",
    "        'R2': 0.0, 'B': 0.0, 'alpha': 0.0,\n",
    "        'empirical_x': None, 'empirical_y': None,\n",
    "        'fit_x': None, 'fit_y': None\n",
    "    }\n",
    "\n",
    "    if total_avalanches < 20:\n",
    "        return results\n",
    "\n",
    "    # 1. Empirical PDF for plotting the scattered blue data.\n",
    "    # We will plot density here as well for consistency.\n",
    "    unique_vals, counts = np.unique(distribution, return_counts=True)\n",
    "    # This is a PMF (Probability Mass Function), not a density, but is standard for scatter plots.\n",
    "    probabilities = counts / total_avalanches\n",
    "    results['empirical_x'] = unique_vals\n",
    "    results['empirical_y'] = probabilities\n",
    "\n",
    "    # 2. Binned data for regression\n",
    "    min_val, max_val = np.min(distribution), np.max(distribution)\n",
    "    if min_val <= 0 or min_val == max_val:\n",
    "        return results # log scale requires positive values\n",
    "\n",
    "    bins = np.logspace(np.log10(min_val), np.log10(max_val), num_bins + 1)\n",
    "    hist_counts, bin_edges = np.histogram(distribution, bins=bins)\n",
    "\n",
    "    # --- CRITICAL FIX: Calculate Probability Density ---\n",
    "\n",
    "    # Calculate the width of each bin\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "\n",
    "    # Calculate density, avoiding division by zero\n",
    "    with warnings.catch_warnings(): # Suppress 'invalid value encountered in true_divide'\n",
    "        warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "        densities = hist_counts / (total_avalanches * bin_widths)\n",
    "\n",
    "    # --- Perform Regression on Density ---\n",
    "    non_zero_mask = densities > 0\n",
    "    densities_for_fit = densities[non_zero_mask]\n",
    "\n",
    "    if len(densities_for_fit) < 3:\n",
    "        return results\n",
    "\n",
    "    bin_centers = ((bin_edges[:-1] + bin_edges[1:]) / 2)[non_zero_mask]\n",
    "\n",
    "    log_x = np.log10(bin_centers)\n",
    "    log_y = np.log10(densities_for_fit)\n",
    "\n",
    "    slope, intercept, r_value, _, _ = linregress(log_x, log_y)\n",
    "\n",
    "    # 3. Store the calculated metrics\n",
    "    results['R2'] = r_value**2 if np.isfinite(r_value) else 0.0\n",
    "    results['B'] = np.sum(hist_counts > 0) / num_bins\n",
    "    # The slope of the density plot is -alpha, so alpha = -slope\n",
    "    results['alpha'] = -slope\n",
    "\n",
    "    # 4. Generate the fitted line for plotting\n",
    "    fit_x_coords = bin_centers\n",
    "\n",
    "    # The line is y = 10^(m*log10(x) + c) in density space\n",
    "    predicted_log_y_density = slope * np.log10(fit_x_coords) + intercept\n",
    "    predicted_y_density = 10**predicted_log_y_density\n",
    "\n",
    "    results['fit_x'] = fit_x_coords\n",
    "\n",
    "    # The y-values of the fit line are densities. We need to convert them to\n",
    "    # probabilities to match the y-axis of the empirical plot (P(x)).\n",
    "    # Prob ~ Density * BinWidth. We use the average bin width for simplicity.\n",
    "    avg_bin_width = np.mean(bin_widths)\n",
    "    results['fit_y'] = predicted_y_density * avg_bin_width * 10 # Heuristic scaling factor for better visualization\n",
    "\n",
    "    # A better approach for the fit line is to plot it in density space, and the empirical data too.\n",
    "    # Let's adjust the empirical data for a more direct comparison.\n",
    "    results['empirical_y'] = probabilities # Keep as PMF for clarity\n",
    "    results['fit_y'] = 10**(slope * np.log10(results['empirical_x']) + intercept) * avg_bin_width # Re-evaluate on empirical x\n",
    "\n",
    "    return results\n"
   ],
   "id": "a66be2acf23b59a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:05:09.851329Z",
     "start_time": "2025-09-30T21:05:09.686660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from skimage import measure\n",
    "import powerlaw\n",
    "\n",
    "# -------------------------\n",
    "# Helpers copied from paper\n",
    "# -------------------------\n",
    "def KSdist(theoretical_pdf, empirical_pdf):\n",
    "    theoretical_pdf = np.asarray(theoretical_pdf, dtype=np.float64)\n",
    "    empirical_pdf   = np.asarray(empirical_pdf,   dtype=np.float64)\n",
    "    if theoretical_pdf.sum() > 0:\n",
    "        theoretical_pdf = theoretical_pdf / theoretical_pdf.sum()\n",
    "    if empirical_pdf.sum() > 0:\n",
    "        empirical_pdf   = empirical_pdf / empirical_pdf.sum()\n",
    "    return float(np.max(np.abs(np.cumsum(theoretical_pdf) - np.cumsum(empirical_pdf))))\n",
    "\n",
    "def getdict_cluster_size(arr1d):\n",
    "    cluster_dict = {}\n",
    "    current_number = None\n",
    "    for a in arr1d:\n",
    "        if current_number == a:\n",
    "            cluster_dict[a][-1] = cluster_dict[a][-1] + 1\n",
    "        else:\n",
    "            current_number = a\n",
    "            if a in cluster_dict:\n",
    "                cluster_dict[a].append(1)\n",
    "            else:\n",
    "                cluster_dict[a] = [1]\n",
    "    return cluster_dict\n",
    "\n",
    "def getarray_avalanche_size(x, value):\n",
    "    list_avalance_size = []\n",
    "    if value in x:\n",
    "        x0size, x1size = x.shape\n",
    "        for i in range(x0size):\n",
    "            if value in x[i, :]:\n",
    "                list_avalance_size.extend(getdict_cluster_size(x[i, :])[value])\n",
    "    return np.array(list_avalance_size)\n",
    "\n",
    "def getarray_avalanche_duration(x, value):\n",
    "    list_avalance_duration = []\n",
    "    if value in x:\n",
    "        x0size, x1size = x.shape\n",
    "        for i in range(x1size):\n",
    "            if value in x[:, i]:\n",
    "                list_avalance_duration.extend(getdict_cluster_size(x[:, i])[value])\n",
    "    return np.array(list_avalance_duration)\n",
    "\n",
    "def norm_ksdist(ksdist, smooth=1):\n",
    "    # exp(- α_D * (0.9*min(mean of first 3, mean of last 3) + 0.1*overall mean))\n",
    "    return float(np.exp(-smooth * (0.9 * min(np.mean(ksdist[:3]), np.mean(ksdist[3:])) + 0.1 * np.mean(ksdist))))\n",
    "\n",
    "def norm_linscore(linscore):\n",
    "    return float(np.mean(linscore))\n",
    "\n",
    "def norm_coef(coef):\n",
    "    return float(-np.mean(coef))\n",
    "\n",
    "def sigmoid(x, smooth=0.01):\n",
    "    return float(1. / (1. + np.exp(-x * smooth)))\n",
    "\n",
    "def norm_R(R_list):\n",
    "    return sigmoid(0.9 * max(np.mean(R_list[:3]), np.mean(R_list[3:])) + 0.1 * np.mean(R_list), smooth=0.01)\n",
    "\n",
    "def normalize_avalanche_pdf_size(mask_avalanche_s_0_bc, mask_avalanche_d_0_bc,\n",
    "                                 mask_avalanche_t_0_bc, mask_avalanche_s_1_bc,\n",
    "                                 mask_avalanche_d_1_bc, mask_avalanche_t_1_bc,\n",
    "                                 width, timesteps):\n",
    "    norm_avalanche_pdf_size_s_0 = np.sum(mask_avalanche_s_0_bc) / width\n",
    "    norm_avalanche_pdf_size_d_0 = np.sum(mask_avalanche_d_0_bc) / timesteps\n",
    "    norm_avalanche_pdf_size_t_0 = np.sum(mask_avalanche_t_0_bc) / (timesteps * width)\n",
    "    norm_avalanche_pdf_size_s_1 = np.sum(mask_avalanche_s_1_bc) / width\n",
    "    norm_avalanche_pdf_size_d_1 = np.sum(mask_avalanche_d_1_bc) / timesteps\n",
    "    norm_avalanche_pdf_size_t_1 = np.sum(mask_avalanche_t_1_bc) / (timesteps * width)\n",
    "\n",
    "    mean_avalanche_pdf_size = np.mean([\n",
    "        norm_avalanche_pdf_size_s_0, norm_avalanche_pdf_size_d_0, norm_avalanche_pdf_size_t_0,\n",
    "        norm_avalanche_pdf_size_s_1, norm_avalanche_pdf_size_d_1, norm_avalanche_pdf_size_t_1\n",
    "    ])\n",
    "\n",
    "    max_avalanche_pdf_size = max(\n",
    "        np.mean([norm_avalanche_pdf_size_s_0, norm_avalanche_pdf_size_d_0, norm_avalanche_pdf_size_t_0]),\n",
    "        np.mean([norm_avalanche_pdf_size_s_1, norm_avalanche_pdf_size_d_1, norm_avalanche_pdf_size_t_1])\n",
    "    )\n",
    "\n",
    "    return float(np.tanh(5 * (0.9 * max_avalanche_pdf_size + 0.1 * mean_avalanche_pdf_size)))\n",
    "\n",
    "def calculate_comparison_ratio(data):\n",
    "    fit = powerlaw.Fit(data, xmin=1, discrete=True)\n",
    "    R_exp, p_exp = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)\n",
    "    return float(R_exp if p_exp < 0.1 else 0.0)\n",
    "\n",
    "# -------------------------\n",
    "# Labeling exactly as paper\n",
    "# -------------------------\n",
    "def get_numbered_avalanches(x, value):\n",
    "    x_value = (x == value).astype(np.int8)\n",
    "    numbered_avalanches_x = measure.label(x_value, background=0)\n",
    "    # Merge wrap-around labels touching both sides\n",
    "    for i in range(numbered_avalanches_x.shape[0]):\n",
    "        if (numbered_avalanches_x[i, 0] != 0 and\n",
    "            numbered_avalanches_x[i, -1] != 0 and\n",
    "            numbered_avalanches_x[i, 0] != numbered_avalanches_x[i, -1]):\n",
    "            numbered_avalanches_x[numbered_avalanches_x == numbered_avalanches_x[i, -1]] = numbered_avalanches_x[i, 0]\n",
    "    return numbered_avalanches_x\n",
    "\n",
    "def getarray_avalanche_size_duration_total(x, value):\n",
    "    numbered_avalanches = get_numbered_avalanches(x, value)\n",
    "    number_of_avalanches = int(np.max(numbered_avalanches))\n",
    "    avalanche_size, avalanche_duration, avalanche_total = [], [], []\n",
    "    for avalanche_number in range(1, number_of_avalanches + 1):\n",
    "        avalanche = np.argwhere(numbered_avalanches == avalanche_number)\n",
    "        if len(avalanche) > 0:\n",
    "            avalanche_duration.append(len(np.unique(avalanche[:, 0])))\n",
    "            avalanche_size.append(len(np.unique(avalanche[:, 1])))\n",
    "            avalanche_total.append(len(avalanche))\n",
    "    return avalanche_size, avalanche_duration, avalanche_total\n",
    "\n",
    "# --------------------------------------\n",
    "# Literal port of evaluate_result(...) —\n",
    "# — but parameterized by width/timesteps\n",
    "# --------------------------------------\n",
    "def evaluate_result_port(ca_result_bin):\n",
    "    \"\"\"\n",
    "    ca_result_bin: 2D uint8/bool array (timesteps x width) with values {0,1}\n",
    "    Returns: fitness (positive), val_dict (same fields as paper)\n",
    "    \"\"\"\n",
    "    timesteps, width = ca_result_bin.shape\n",
    "\n",
    "    # Collect avalanches for both states and all three measures\n",
    "    avalanche_s_0, avalanche_d_0, avalanche_t_0 = getarray_avalanche_size_duration_total(ca_result_bin, 0)\n",
    "    avalanche_s_1, avalanche_d_1, avalanche_t_1 = getarray_avalanche_size_duration_total(ca_result_bin, 1)\n",
    "\n",
    "    # Bin counts, drop zero bin, require >5 samples as in paper paths\n",
    "    avalanche_s_0_bc = np.bincount(avalanche_s_0)[1:] if len(avalanche_s_0) > 5 else np.array([])\n",
    "    avalanche_d_0_bc = np.bincount(avalanche_d_0)[1:] if len(avalanche_d_0) > 5 else np.array([])\n",
    "    avalanche_t_0_bc = np.bincount(avalanche_t_0)[1:] if len(avalanche_t_0) > 5 else np.array([])\n",
    "\n",
    "    avalanche_s_1_bc = np.bincount(avalanche_s_1)[1:] if len(avalanche_s_1) > 5 else np.array([])\n",
    "    avalanche_d_1_bc = np.bincount(avalanche_d_1)[1:] if len(avalanche_d_1) > 5 else np.array([])\n",
    "    avalanche_t_1_bc = np.bincount(avalanche_t_1)[1:] if len(avalanche_t_1) > 5 else np.array([])\n",
    "\n",
    "    # Normalize to PDFs (paper does this)\n",
    "    def _to_pdf(bc):\n",
    "        bc = np.asarray(bc, dtype=np.float64)\n",
    "        s = bc.sum()\n",
    "        return bc / s if s > 0 else bc\n",
    "\n",
    "    avalanche_s_0_bc = _to_pdf(avalanche_s_0_bc)\n",
    "    avalanche_d_0_bc = _to_pdf(avalanche_d_0_bc)\n",
    "    avalanche_t_0_bc = _to_pdf(avalanche_t_0_bc)\n",
    "    avalanche_s_1_bc = _to_pdf(avalanche_s_1_bc)\n",
    "    avalanche_d_1_bc = _to_pdf(avalanche_d_1_bc)\n",
    "    avalanche_t_1_bc = _to_pdf(avalanche_t_1_bc)\n",
    "\n",
    "    # Masks for non-zero bins (paper uses these)\n",
    "    mask_avalanche_s_0_bc = avalanche_s_0_bc > 0\n",
    "    mask_avalanche_d_0_bc = avalanche_d_0_bc > 0\n",
    "    mask_avalanche_t_0_bc = avalanche_t_0_bc > 0\n",
    "    mask_avalanche_s_1_bc = avalanche_s_1_bc > 0\n",
    "    mask_avalanche_d_1_bc = avalanche_d_1_bc > 0\n",
    "    mask_avalanche_t_1_bc = avalanche_t_1_bc > 0\n",
    "\n",
    "    # log10 PDFs, zeros where masked out (exactly like the paper)\n",
    "    def _log_masked(pdf, mask):\n",
    "        logp = np.zeros_like(pdf)\n",
    "        nz = mask\n",
    "        logp[nz] = np.log10(pdf[nz])\n",
    "        return logp\n",
    "\n",
    "    log_avalanche_s_0_bc = _log_masked(avalanche_s_0_bc, mask_avalanche_s_0_bc)\n",
    "    log_avalanche_d_0_bc = _log_masked(avalanche_d_0_bc, mask_avalanche_d_0_bc)\n",
    "    log_avalanche_t_0_bc = _log_masked(avalanche_t_0_bc, mask_avalanche_t_0_bc)\n",
    "    log_avalanche_s_1_bc = _log_masked(avalanche_s_1_bc, mask_avalanche_s_1_bc)\n",
    "    log_avalanche_d_1_bc = _log_masked(avalanche_d_1_bc, mask_avalanche_d_1_bc)\n",
    "    log_avalanche_t_1_bc = _log_masked(avalanche_t_1_bc, mask_avalanche_t_1_bc)\n",
    "\n",
    "    # --- Hard gate: need >5 non-zero bins among first 10, for all six dists ---\n",
    "    def _left10_nonzero(mask):\n",
    "        return int(np.sum(mask[:10]))\n",
    "\n",
    "    if not ( _left10_nonzero(mask_avalanche_s_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_d_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_t_0_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_s_1_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_d_1_bc) > 5 and\n",
    "             _left10_nonzero(mask_avalanche_t_1_bc) > 5 ):\n",
    "        # Paper returns effectively zero fitness if this fails\n",
    "        return 0.0, {\n",
    "            \"norm_ksdist_res\": 0.0,\n",
    "            \"norm_coef_res\":   0.0,\n",
    "            \"norm_unique_states\": 0.0,\n",
    "            \"norm_avalanche_pdf_size\": 0.0,\n",
    "            \"norm_linscore_res\": 0.0,\n",
    "            \"norm_R_res\": 0.0,\n",
    "            \"fitness\": 0.0,\n",
    "        }\n",
    "\n",
    "    # --- Fit log–log lines (unweighted R^2), then refit with sample_weight for first 10 bins ---\n",
    "    def _fit_lin(pdf, log_pdf, mask):\n",
    "        xs = np.log10(np.arange(1, len(pdf) + 1)[mask]).reshape(-1, 1)\n",
    "        ys = log_pdf[mask]\n",
    "        if xs.shape[0] == 0:\n",
    "            return None, 0.0\n",
    "        lr = LinearRegression().fit(xs, ys)\n",
    "        r2 = lr.score(xs, ys)\n",
    "        # re-fit with weights that are 1 for bins <10, else 0 (paper’s weighted fit)\n",
    "        idxs_all = np.arange(len(pdf))[mask]\n",
    "        w = np.array([1 if idx < 10 else 0 for idx in idxs_all], dtype=float)\n",
    "        lr_w = LinearRegression().fit(xs, ys, sample_weight=w)\n",
    "        return (lr, lr_w)\n",
    "\n",
    "    fits = []\n",
    "    for pdf, logp, mask in [\n",
    "        (avalanche_s_0_bc, log_avalanche_s_0_bc, mask_avalanche_s_0_bc),\n",
    "        (avalanche_d_0_bc, log_avalanche_d_0_bc, mask_avalanche_d_0_bc),\n",
    "        (avalanche_t_0_bc, log_avalanche_t_0_bc, mask_avalanche_t_0_bc),\n",
    "        (avalanche_s_1_bc, log_avalanche_s_1_bc, mask_avalanche_s_1_bc),\n",
    "        (avalanche_d_1_bc, log_avalanche_d_1_bc, mask_avalanche_d_1_bc),\n",
    "        (avalanche_t_1_bc, log_avalanche_t_1_bc, mask_avalanche_t_1_bc),\n",
    "    ]:\n",
    "        lr, lr_w = _fit_lin(pdf, logp, mask)\n",
    "        fits.append((pdf, logp, mask, lr, lr_w))\n",
    "\n",
    "    # R^2 list from the unweighted fits\n",
    "    linscore_list = []\n",
    "    for pdf, logp, mask, lr, lr_w in fits:\n",
    "        xs = np.log10(np.arange(1, len(pdf) + 1)[mask]).reshape(-1, 1)\n",
    "        ys = logp[mask]\n",
    "        linscore_list.append(lr.score(xs, ys))\n",
    "\n",
    "    # Theoretical PDFs from the weighted fits (full support, not only masked bins)\n",
    "    def _theor_from_fit(lr_w, L):\n",
    "        xs_full = np.log10(np.arange(1, L + 1).reshape(-1, 1))\n",
    "        pred = lr_w.predict(xs_full)  # log10(pdf)\n",
    "        theor = np.power(10.0, pred)\n",
    "        return theor\n",
    "\n",
    "    theor_avalanche_s_0_bc = _theor_from_fit(fits[0][4], len(avalanche_s_0_bc))\n",
    "    theor_avalanche_d_0_bc = _theor_from_fit(fits[1][4], len(avalanche_d_0_bc))\n",
    "    theor_avalanche_t_0_bc = _theor_from_fit(fits[2][4], len(avalanche_t_0_bc))\n",
    "    theor_avalanche_s_1_bc = _theor_from_fit(fits[3][4], len(avalanche_s_1_bc))\n",
    "    theor_avalanche_d_1_bc = _theor_from_fit(fits[4][4], len(avalanche_d_1_bc))\n",
    "    theor_avalanche_t_1_bc = _theor_from_fit(fits[5][4], len(avalanche_t_1_bc))\n",
    "\n",
    "    # KS against the fitted curves\n",
    "    ksdist_list = [\n",
    "        KSdist(theor_avalanche_s_0_bc, avalanche_s_0_bc),\n",
    "        KSdist(theor_avalanche_d_0_bc, avalanche_d_0_bc),\n",
    "        KSdist(theor_avalanche_t_0_bc, avalanche_t_0_bc),\n",
    "        KSdist(theor_avalanche_s_1_bc, avalanche_s_1_bc),\n",
    "        KSdist(theor_avalanche_d_1_bc, avalanche_d_1_bc),\n",
    "        KSdist(theor_avalanche_t_1_bc, avalanche_t_1_bc),\n",
    "    ]\n",
    "\n",
    "    # Coefs (slopes) from the weighted fits\n",
    "    coef_list = [\n",
    "        fits[0][4].coef_[0], fits[1][4].coef_[0], fits[2][4].coef_[0],\n",
    "        fits[3][4].coef_[0], fits[4][4].coef_[0], fits[5][4].coef_[0],\n",
    "    ]\n",
    "\n",
    "    # B (coverage of first-10 bins), exactly as in paper's normalize function\n",
    "    norm_avalanche_pdf_size = normalize_avalanche_pdf_size(\n",
    "        fits[0][2], fits[1][2], fits[2][2], fits[3][2], fits[4][2], fits[5][2],\n",
    "        width=width, timesteps=timesteps\n",
    "    )\n",
    "\n",
    "    # Map components to [0,1] in the same way\n",
    "    norm_linscore_res   = norm_linscore(linscore_list)\n",
    "    norm_ksdist_res     = norm_ksdist(ksdist_list)\n",
    "    norm_coef_res       = norm_coef(coef_list)\n",
    "    norm_unique_states  = (np.unique(ca_result_bin, axis=0).shape[0]) / float(width)\n",
    "\n",
    "    # Final fitness (positive)\n",
    "    fitness = (norm_ksdist_res ** 2) + norm_unique_states + norm_avalanche_pdf_size + (norm_linscore_res ** 2)\n",
    "\n",
    "    # Optional L̂ term (power law vs exponential); only if fitness > 3.0, like the paper\n",
    "    norm_R_res = 0.0\n",
    "    if fitness > 3.0:\n",
    "        R_list = [\n",
    "            calculate_comparison_ratio(getarray_avalanche_size(ca_result_bin, 0)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_duration(ca_result_bin, 0)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size_duration_total(ca_result_bin, 0)[2]),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size(ca_result_bin, 1)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_duration(ca_result_bin, 1)),\n",
    "            calculate_comparison_ratio(getarray_avalanche_size_duration_total(ca_result_bin, 1)[2]),\n",
    "        ]\n",
    "        norm_R_res = norm_R(R_list)\n",
    "        fitness += norm_R_res\n",
    "\n",
    "    val_dict = {\n",
    "        \"norm_ksdist_res\":      float(norm_ksdist_res),\n",
    "        \"norm_coef_res\":        float(norm_coef_res),\n",
    "        \"norm_unique_states\":   float(norm_unique_states),\n",
    "        \"norm_avalanche_pdf_size\": float(norm_avalanche_pdf_size),\n",
    "        \"norm_linscore_res\":    float(norm_linscore_res),\n",
    "        \"norm_R_res\":           float(norm_R_res),\n",
    "        \"fitness\":              float(fitness),\n",
    "    }\n",
    "    return float(fitness), val_dict"
   ],
   "id": "a2b49e4b89cd9fb8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T21:05:09.901559Z",
     "start_time": "2025-09-30T21:05:09.887578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CEM():\n",
    "    def __init__(self, N, T, dt, alpha_m):\n",
    "\n",
    "        # Local polynomial coefficients (theta_*):\n",
    "        # dot a_i = theta1 + theta_c*c + theta_l*l + theta_r*r + theta_m*m\n",
    "        #           + theta_cl*c*l + theta_cr*c*r + theta_cm*c*m\n",
    "        self.theta = {\n",
    "            \"theta1\": random.uniform(-0.1755730629920966,0.2178503719520462),\n",
    "            \"theta_c\": random.uniform(-0.1755730629920966,0.2178503719520462),\n",
    "            \"theta_l\":  random.uniform(-0.1118137492986643,0.2496046008954666),\n",
    "            \"theta_r\":  random.uniform(-0.1213590547976200,0.2095647696014324),\n",
    "            \"theta_m\":  random.uniform(-0.1073989561467485,0.1962269502658598),\n",
    "            \"theta_cl\": random.uniform(-0.0403281277587537,0.0666194993654684),\n",
    "            \"theta_cr\": random.uniform(-0.0531497670807622,0.0933092276640812),\n",
    "            \"theta_cm\": random.uniform(-0.1778159966117346,-0.0347120789791888),  # small negative helps soft saturation\n",
    "        }\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.alpha_m = alpha_m\n",
    "\n",
    "       # RNG seed for reproducible initial state\n",
    "        self.A, self.M = self.__init_state__()\n",
    "        self.fitness = -1e3\n",
    "        self.fit_res = None\n",
    "\n",
    "\n",
    "\n",
    "    def __init_state__(self, state = None):\n",
    "        if state is None:\n",
    "            init_scale = 1 # amplitude of random initialization\n",
    "            rng = np.random.default_rng(42)\n",
    "            a0 = init_scale * rng.standard_normal(self.N)\n",
    "            m0 = init_scale * rng.standard_normal(self.N)\n",
    "\n",
    "            A = np.empty((self.T+1, self.N), dtype=float)\n",
    "            M = np.empty((self.T+1, self.N), dtype=float)\n",
    "            A[0] = a0\n",
    "            M[0] = m0\n",
    "\n",
    "            return A,M\n",
    "        if state is not None:\n",
    "            self.A[0], self.M[0] = state\n",
    "\n",
    "# %%\n",
    "# ---- Dynamics (periodic boundary; RK4 integrator) ----\n",
    "\n",
    "\n",
    "    def deriv(self,a, m):\n",
    "        \"\"\"Compute time-derivatives (dot a, dot m) given current (a, m).\"\"\"\n",
    "        l = np.roll(a, 1)   # a_{i-1}\n",
    "        r = np.roll(a, -1)  # a_{i+1}\n",
    "        c = a\n",
    "        t = self.theta\n",
    "        dot_a = (t[\"theta1\"]\n",
    "                 + t[\"theta_c\"]*c + t[\"theta_l\"]*l + t[\"theta_r\"]*r + t[\"theta_m\"]*m\n",
    "                 + t[\"theta_cl\"]*c*l + t[\"theta_cr\"]*c*r + t[\"theta_cm\"]*c*m) - 5*self.alpha_m*c\n",
    "        dot_m = self.alpha_m * (a - m)\n",
    "        return dot_a, dot_m\n",
    "\n",
    "    def rk4_step(self,a, m):\n",
    "        \"\"\"One RK4 step for the coupled ODEs.\"\"\"\n",
    "        k1_a, k1_m = self.deriv(a, m)\n",
    "        k2_a, k2_m = self.deriv(a + 0.5*self.dt*k1_a, m + 0.5*self.dt*k1_m)\n",
    "        k3_a, k3_m = self.deriv(a + 0.5*self.dt*k2_a, m + 0.5*self.dt*k2_m)\n",
    "        k4_a, k4_m = self.deriv(a + self.dt*k3_a,     m + self.dt*k3_m)\n",
    "        a_next = a + (self.dt/6.0)*(k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
    "        m_next = m + (self.dt/6.0)*(k1_m + 2*k2_m + 2*k3_m + k4_m)\n",
    "        return a_next, m_next\n",
    "\n",
    "    def simulate(self):\n",
    "        for t in range(1, self.T+1):\n",
    "            a,m = self.A[t-1], self.M[t-1]\n",
    "            a, m = self.rk4_step(a, m)\n",
    "            self.A[t] = a\n",
    "            self.M[t] = m\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "    def compute_criticality_metric_numpy(self,A,\n",
    "                                     threshold_z_score=1.5,\n",
    "                                     target_alpha=1.5,\n",
    "                                     sigma_alpha=0.5,\n",
    "                                     return_plot_data=False):\n",
    "        \"\"\"\n",
    "        Computes a criticality metric using a pure NumPy/SciPy power-law fit.\n",
    "        Optionally returns data required for plotting the distribution and the fit.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            activity_mean = np.mean(A)\n",
    "            activity_std = np.std(A)\n",
    "            if activity_std == 0:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            threshold = activity_mean + threshold_z_score * activity_std\n",
    "            binarized_A = A > 0\n",
    "\n",
    "            labeled_array, num_avalanches = label(binarized_A)\n",
    "            if num_avalanches < 2:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            avalanche_sizes = np.bincount(labeled_array.ravel())[1:]\n",
    "\n",
    "            if len(avalanche_sizes) < 30:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            # Call our fitter, which now returns a dictionary\n",
    "            fit_results = fit_power_law_numpy(avalanche_sizes)\n",
    "            self.fit_res = fit_results\n",
    "            if not fit_results['fit_successful']:\n",
    "                return 0.0 if not return_plot_data else (0.0, fit_results)\n",
    "\n",
    "            # Calculate fitness from the results\n",
    "            goodness_of_fit_score = 1.0 - fit_results['ks_distance']\n",
    "            slope_score = np.exp(-((fit_results['alpha'] - target_alpha)**2) / (2 * sigma_alpha**2))\n",
    "            final_fitness = max(0, goodness_of_fit_score * slope_score)\n",
    "\n",
    "            # Return either just the fitness or the fitness and the plot data\n",
    "            if return_plot_data:\n",
    "                return final_fitness, fit_results\n",
    "            else:\n",
    "                return final_fitness\n",
    "\n",
    "        except Exception:\n",
    "            return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_fitness(self, bound=10.0, penalty_strength=2.0):\n",
    "        \"\"\"\n",
    "        Runs simulate(), scores with criticality_fitness(), and applies a soft\n",
    "        penalty if any cells in A leave [-bound, bound]. Robust to NaNs/Infs.\n",
    "        \"\"\"\n",
    "        self.simulate()\n",
    "\n",
    "        A = self.A\n",
    "        # If the sim blew up, assign zero fitness.\n",
    "        if not np.all(np.isfinite(A)):\n",
    "            self.fitness = 0.0\n",
    "            return self.fitness\n",
    "\n",
    "        # Base score (guard NaNs)\n",
    "        base = self.compute_criticality_metric_numpy(A)\n",
    "        if not np.isfinite(base):\n",
    "            base = 0.0\n",
    "\n",
    "        # Soft violation metric: how far |A| exceeds 'bound'\n",
    "        # excess[i,t] = max(0, |A| - bound)\n",
    "        excess = np.maximum(0.0, np.abs(A) - float(bound))\n",
    "        if excess.size == 0:\n",
    "            self.fitness = float(base)\n",
    "            return self.fitness\n",
    "\n",
    "        v_mean = float(np.mean(excess))\n",
    "        v_max  = float(np.max(excess))\n",
    "        # Combine mean + max so brief spikes are punished too\n",
    "        v = 0.7 * v_mean + 0.3 * v_max\n",
    "\n",
    "        # Exponential penalty factor in (0,1]; stronger with larger 'penalty_strength'\n",
    "        penalty = float(np.exp(-penalty_strength * v))\n",
    "\n",
    "        # Optional hard fail if it really blows past the bound (comment out if undesired)\n",
    "        if v_max > bound:  # e.g., any cell beyond ±(bound+bound) ⇒ zero\n",
    "            penalty = 0.0\n",
    "\n",
    "        self.fitness = float(base * penalty)\n",
    "        return self.fitness\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "#-------------------------------------------------------------#\n",
    "class CEM1Layer:\n",
    "\n",
    "    def __init__(self, N, T, dt, alpha_m,\n",
    "                 theta=None, gamma=0.0, rng_seed=42, init_scale=1.0):\n",
    "        self.N = int(N)\n",
    "        self.T = int(T)\n",
    "        self.dt = float(dt)\n",
    "        self.alpha_m = float(alpha_m)   # kept for interface compatibility\n",
    "        self.gamma = float(gamma)\n",
    "\n",
    "\n",
    "\n",
    "        if theta is None:\n",
    "            self.theta = {\n",
    "                \"k0\": random.uniform(-0.10,  0.10),\n",
    "                \"k1\": random.uniform(-0.40,  0.40),\n",
    "                \"k2\": random.uniform(-0.60,  0.60),\n",
    "                \"k3\": random.uniform(-0.40,  0.40),\n",
    "                \"k4\": random.uniform(-0.30,  0.30),\n",
    "                \"k5\": random.uniform(-0.30,  0.30),\n",
    "                \"k6\": random.uniform(-0.15,  0.15),\n",
    "            }\n",
    "        else:\n",
    "            keys = [\"k0\",\"k1\",\"k2\",\"k3\",\"k4\",\"k5\",\"k6\"]\n",
    "            self.theta = {k: float(theta.get(k, 0.0)) for k in keys}\n",
    "\n",
    "        # Allocate state history buffers to mirror your class\n",
    "        self.A = np.empty((self.T + 1, self.N), dtype=float)\n",
    "\n",
    "\n",
    "        # Fitness bookkeeping (mirrors your attributes)\n",
    "        self.fitness = -1e3\n",
    "        self.fit_res = None\n",
    "\n",
    "        # Initialize state (A[0]) like your __init_state__\n",
    "        self.__init_state__(state=None, rng_seed=rng_seed, init_scale=init_scale)\n",
    "\n",
    "    # ---- Initialization (mirrors your signature/behavior) ----\n",
    "    def __init_state__(self, state=None, rng_seed=42, init_scale=1.0):\n",
    "        \"\"\"\n",
    "        If state is None: random normal initial A[0].\n",
    "        If state is a tuple/list (A0, M0) from old 2-layer code, we use only A0.\n",
    "        If state is a 1D array of shape (N,), we use it as A[0].\n",
    "        \"\"\"\n",
    "        \"\"\"if state is None:\n",
    "            rng = np.random.default_rng(rng_seed)\n",
    "            a0 = init_scale * rng.standard_normal(self.N)\n",
    "            self.A[0] = a0\n",
    "            return self.A\"\"\"\n",
    "\n",
    "        if state is None:\n",
    "            num_state = self.N//10\n",
    "            inputs  = np.random.randint(0,self.N,num_state)\n",
    "            a0 = np.zeros(self.N, dtype=float)\n",
    "            a0[inputs] = 1.0\n",
    "        else:\n",
    "            a0 = np.asarray(state, dtype=float)\n",
    "\n",
    "        assert a0.shape == (self.N,), f\"Initial state must be shape (N,), got {a0.shape}\"\n",
    "        self.A[0] = a0\n",
    "        return self.A\n",
    "\n",
    "    # ---- Dynamics (periodic boundary; RK4 integrator) ----\n",
    "    def deriv(self, a):\n",
    "        \"\"\"Compute ds/dt for the current state a (periodic neighborhood).\"\"\"\n",
    "        l = np.roll(a,  1)\n",
    "        r = np.roll(a, -1)\n",
    "        c = a\n",
    "        t = self.theta\n",
    "        dot = (t[\"k0\"]*0.0\n",
    "               + t[\"k1\"]*l + t[\"k2\"]*c + t[\"k3\"]*r\n",
    "               + t[\"k4\"]*(c*r) + t[\"k5\"]*(c*l)\n",
    "               + t[\"k6\"]*(l*c*r)\n",
    "               - self.gamma * c)\n",
    "        return dot\n",
    "\n",
    "    def rk4_step(self, a):\n",
    "        \"\"\"One RK4 step for ds/dt = f(a).\"\"\"\n",
    "        k1 = self.deriv(a)\n",
    "        k2 = self.deriv(a + 0.5*self.dt*k1)\n",
    "        k3 = self.deriv(a + 0.5*self.dt*k2)\n",
    "        k4 = self.deriv(a + self.dt*k3)\n",
    "        a_next = a + (self.dt/6.0)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "        return a_next\n",
    "\n",
    "\n",
    "    def simulate(self):\n",
    "        \"\"\"Run T integration steps, filling self.A[1:].\"\"\"\n",
    "        for t in range(1, self.T + 1):\n",
    "            a_prev = self.A[t - 1]\n",
    "            a_next = self.rk4_step(a_prev)\n",
    "            self.A[t] = a_next\n",
    "\n",
    "    # ---- Helpers to mirror ergonomics ----\n",
    "    def set_theta(self, **kwargs):\n",
    "        \"\"\"Update any subset of k0..k6 (e.g., set_theta(k2=0.1, k6=-0.05)).\"\"\"\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in self.theta:\n",
    "                raise KeyError(f\"Unknown coefficient '{k}'. Valid keys: {list(self.theta.keys())}\")\n",
    "            self.theta[k] = float(v)\n",
    "\n",
    "    def get_theta(self):\n",
    "        return dict(self.theta)\n",
    "\n",
    "\n",
    "    def compute_criticality_metric_numpy(self,A,\n",
    "                                     threshold_z_score=1.5,\n",
    "                                     target_alpha=1.5,\n",
    "                                     sigma_alpha=0.5,\n",
    "                                     return_plot_data=False):\n",
    "        \"\"\"\n",
    "        Computes a criticality metric using a pure NumPy/SciPy power-law fit.\n",
    "        Optionally returns data required for plotting the distribution and the fit.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            activity_mean = np.mean(A)\n",
    "            activity_std = np.std(A)\n",
    "            if activity_std == 0:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            threshold = activity_mean + threshold_z_score * activity_std\n",
    "            binarized_A = A > 0\n",
    "\n",
    "            labeled_array, num_avalanches = label(binarized_A)\n",
    "            if num_avalanches < 2:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            avalanche_sizes = np.bincount(labeled_array.ravel())[1:]\n",
    "\n",
    "            if len(avalanche_sizes) < 30:\n",
    "                return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "            # Call our fitter, which now returns a dictionary\n",
    "            fit_results = fit_power_law_numpy(avalanche_sizes)\n",
    "            self.fit_res = fit_results\n",
    "            if not fit_results['fit_successful']:\n",
    "                return 0.0 if not return_plot_data else (0.0, fit_results)\n",
    "\n",
    "            # Calculate fitness from the results\n",
    "            goodness_of_fit_score = 1.0 - fit_results['ks_distance']\n",
    "            slope_score = np.exp(-((fit_results['alpha'] - target_alpha)**2) / (2 * sigma_alpha**2))\n",
    "            final_fitness = max(0, goodness_of_fit_score * slope_score)\n",
    "\n",
    "            # Return either just the fitness or the fitness and the plot data\n",
    "            if return_plot_data:\n",
    "                return final_fitness, fit_results\n",
    "            else:\n",
    "                return final_fitness\n",
    "\n",
    "        except Exception:\n",
    "            return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "    def compute_criticality_from_paper(self,A, return_plot_data=False):\n",
    "        \"\"\"\n",
    "        Computes the criticality fitness score as described by Pontes-Filho et al.\n",
    "        Optionally returns data required for plotting the six avalanche distributions.\n",
    "        \"\"\"\n",
    "        # Threshold A at 0 to get a binary matrix\n",
    "        binarized_A = (A > 0).astype(int)\n",
    "\n",
    "        # 1. Get avalanche distributions (same as before)\n",
    "        sizes0, durations0, areas0 = _get_avalanches(binarized_A, state=0)\n",
    "        sizes1, durations1, areas1 = _get_avalanches(binarized_A, state=1)\n",
    "        all_distributions = [sizes0, durations0, areas0, sizes1, durations1, areas1]\n",
    "\n",
    "        if any(len(dist) == 0 for dist in all_distributions):\n",
    "            return 0.0 if not return_plot_data else (0.0, None)\n",
    "\n",
    "        # 2. Calculate metrics and gather plot data\n",
    "        R2_values = []\n",
    "        B_values = []\n",
    "        plot_data_list = []\n",
    "\n",
    "        for dist in all_distributions:\n",
    "            metrics_and_plot_data = _calculate_metrics_from_dist(dist)\n",
    "            R2_values.append(metrics_and_plot_data['R2'])\n",
    "            B_values.append(metrics_and_plot_data['B'])\n",
    "            plot_data_list.append(metrics_and_plot_data)\n",
    "\n",
    "        R2_values = np.nan_to_num(R2_values)\n",
    "        B_values = np.nan_to_num(B_values)\n",
    "\n",
    "        # 3. Combine scores (same logic as before)\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        alpha_R2, alpha_B = 0.01, 5.0\n",
    "\n",
    "        R2_0_avg, R2_1_avg = np.mean(R2_values[:3]), np.mean(R2_values[3:])\n",
    "        B_0_avg, B_1_avg = np.mean(B_values[:3]), np.mean(B_values[3:])\n",
    "\n",
    "        R2_score = sigmoid(alpha_R2 * (0.9 * max(R2_0_avg, R2_1_avg) + 0.1 * np.mean(R2_values)))\n",
    "        B_score = np.tanh(alpha_B * (0.9 * max(B_0_avg, B_1_avg) + 0.1 * np.mean(B_values)))\n",
    "\n",
    "        mean_state = np.mean(binarized_A)\n",
    "        U_score = 1.0 - np.abs(mean_state - 0.5) * 2\n",
    "\n",
    "        S_final = (R2_score)**2 + B_score + U_score\n",
    "\n",
    "        # 4. Return the results\n",
    "        if return_plot_data:\n",
    "            return S_final, plot_data_list\n",
    "        else:\n",
    "            self.fit_res = plot_data_list\n",
    "            return S_final\n",
    "\n",
    "    def cem_criticality_fitness_port(self,A_continuous):\n",
    "        \"\"\"\n",
    "        A_continuous: np.ndarray of shape (T+1, N) or (T, N), real-valued from CEM.\n",
    "        Returns positive fitness (maximize this).\n",
    "        This is a literal port of the paper’s evaluator, operating on A>0 binarization.\n",
    "        \"\"\"\n",
    "        A = np.asarray(A_continuous)\n",
    "        if A.ndim != 2:\n",
    "            raise ValueError(f\"A must be 2D (T x N), got shape {A.shape}\")\n",
    "        # If you pass (T+1,N), that's fine — the paper just treats rows as timesteps.\n",
    "        Abin = (A > 0).astype(np.uint8)\n",
    "        fitness, _ = evaluate_result_port(Abin)\n",
    "        return float(fitness)\n",
    "\n",
    "\n",
    "    def compute_fitness(self, bound=3.0, penalty_strength=2.0):\n",
    "        \"\"\"\n",
    "        Runs simulate(), scores with criticality_fitness(), and applies a soft\n",
    "        penalty if any cells in A leave [-bound, bound]. Robust to NaNs/Infs.\n",
    "        \"\"\"\n",
    "        self.simulate()\n",
    "\n",
    "        A = self.A\n",
    "        # If the sim blew up, assign zero fitness.\n",
    "        if not np.all(np.isfinite(A)):\n",
    "            self.fitness = -10000.0\n",
    "            return self.fitness\n",
    "\n",
    "        # Base score (guard NaNs)\n",
    "        base = self.compute_criticality_metric_numpy(A)\n",
    "        #base = self.compute_criticality_from_paper(A)\n",
    "        base2= self.cem_criticality_fitness_port(A)\n",
    "        base += base2\n",
    "        if not np.isfinite(base):\n",
    "            base = -10000.0\n",
    "\n",
    "        # Soft violation metric: how far |A| exceeds 'bound'\n",
    "        # excess[i,t] = max(0, |A| - bound)\n",
    "        excess = np.maximum(0.0, np.abs(A) - float(bound))\n",
    "        if excess.size == 0:\n",
    "            self.fitness = float(base)\n",
    "            return self.fitness\n",
    "\n",
    "        v_mean = float(np.mean(excess))\n",
    "        v_max  = float(np.max(excess))\n",
    "        # Combine mean + max so brief spikes are punished too\n",
    "        v = 0.7 * v_mean + 0.3 * v_max\n",
    "\n",
    "        # Exponential penalty factor in (0,1]; stronger with larger 'penalty_strength'\n",
    "        penalty = float(np.exp(-penalty_strength * v))\n",
    "\n",
    "        # Optional hard fail if it really blows past the bound (comment out if undesired)\n",
    "        if v_max > bound:  # e.g., any cell beyond ±(bound+bound) ⇒ zero\n",
    "            penalty = 0.0\n",
    "\n",
    "        self.fitness = float(base * penalty)\n",
    "        return self.fitness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:05:09.985357Z",
     "start_time": "2025-09-30T21:05:09.944205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np, random, matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ---------- genetics ----------\n",
    "def crossover_pointwise(t1, t2, rate=1.0):\n",
    "    if random.random() > rate:  # skip cx\n",
    "        return dict(t1)\n",
    "    return {k: (t1[k] if random.random()<0.5 else t2[k]) for k in t1.keys()}\n",
    "\n",
    "def mutate_with_sigma(theta, sigma, rate=0.2):\n",
    "    \"\"\"Per-gene mutation with prob=rate; Gaussian scale = sigma*(|v|+1).\"\"\"\n",
    "    out = {}\n",
    "    for k, v in theta.items():\n",
    "        if random.random() < rate:\n",
    "            out[k] = v + np.random.normal(0.0, sigma*(abs(v)+1.0))\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def mutate_sigma(sigma, tau=0.2):\n",
    "    \"\"\"Log-normal self-adaptation: sigma' = sigma * exp(tau * N(0,1)).\"\"\"\n",
    "    return max(1e-8, float(sigma * np.exp(tau * np.random.randn())))\n",
    "\n",
    "# ---------- selection with coverage + elitism ----------\n",
    "def select_mu_with_coverage_and_elites(pool, mu, k=3, elites=1):\n",
    "    # pool: list of dicts {'theta','fitness','sigma'}\n",
    "    pool_sorted = sorted(pool, key=lambda z: z[\"fitness\"], reverse=True)\n",
    "    parents, taken = pool_sorted[:elites], set()\n",
    "    for p in parents: taken.add(id(p))\n",
    "    n = len(pool)\n",
    "    idx = list(range(n)); random.shuffle(idx)\n",
    "    r = (-len(idx)) % k; idx += random.choices(range(n), k=r)  # pad\n",
    "    for i in range(0, len(idx), k):\n",
    "        cand = [pool[j] for j in idx[i:i+k]]\n",
    "        win = max(cand, key=lambda z: z[\"fitness\"])\n",
    "        if id(win) not in taken:\n",
    "            parents.append(win); taken.add(id(win))\n",
    "        if len(parents) == mu: break\n",
    "    if len(parents) < mu:\n",
    "        for z in pool_sorted:\n",
    "            if id(z) not in taken:\n",
    "                parents.append(z); taken.add(id(z))\n",
    "            if len(parents) == mu: break\n",
    "    return parents[:mu]\n",
    "\n",
    "def make_pairs_with_coverage(parents, num_pairs):\n",
    "    P = parents[:]\n",
    "    pairs = []\n",
    "    while len(pairs) < num_pairs:\n",
    "        random.shuffle(P)\n",
    "        base = [(P[i], P[(i+1) % len(P)]) for i in range(len(P))]\n",
    "        pairs.extend(base)\n",
    "    return pairs[:num_pairs]\n",
    "\n",
    "# ---------- parallel evaluation ----------\n",
    "def _worker_eval(theta, N, T, dt, alpha_m):\n",
    "    cem = CEM1Layer(N, T, dt, alpha_m)\n",
    "    cem.theta = dict(theta)\n",
    "    cem.compute_fitness()\n",
    "    return dict(theta=cem.theta, fitness=float(cem.fitness))\n",
    "\n",
    "def _parallel_eval(theta_list, N, T, dt, alpha_m, n_jobs):\n",
    "    if n_jobs == 1:\n",
    "        return [_worker_eval(th, N, T, dt, alpha_m) for th in theta_list]\n",
    "    out = []\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "        futs = [ex.submit(_worker_eval, th, N, T, dt, alpha_m) for th in theta_list]\n",
    "        for f in as_completed(futs):\n",
    "            out.append(f.result())\n",
    "    return out\n",
    "\n",
    "# ---------- main EA (self-adaptive σ) ----------\n",
    "def mu_plus_lambda_ea_parallel_sa(\n",
    "    N=128, T=256, dt=0.3, alpha_m=0.05,\n",
    "    mu=20, lam=40, generations=50,\n",
    "    tournament_k=3, cx_rate=1.0, mut_rate=0.2,\n",
    "    elites=1, base_sigma=0.08, tau_sigma=0.2,\n",
    "    n_jobs=None, show=True, seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-adaptive σ:\n",
    "      - Each individual has 'sigma'\n",
    "      - child_sigma = geom_mean(parent sigmas) then log-normal mutate by tau_sigma\n",
    "      - θ mutation uses that sigma\n",
    "    Rest of the loop/order unchanged (μ+λ with coverage + elitism).\n",
    "    \"\"\"\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    if n_jobs is None:\n",
    "        try:\n",
    "            import os; n_jobs = max(1, os.cpu_count() or 1)\n",
    "        except Exception:\n",
    "            n_jobs = 1\n",
    "\n",
    "    # init μ+λ thetas & attach σ\n",
    "    init_cems = [CEM1Layer(N, T, dt, alpha_m) for _ in range(mu + lam)]\n",
    "    pool = _parallel_eval([c.theta for c in init_cems], N, T, dt, alpha_m, n_jobs=n_jobs)\n",
    "    for z in pool: z[\"sigma\"] = float(base_sigma)\n",
    "\n",
    "    if show: plt.figure(figsize=(6,3))\n",
    "\n",
    "    for g in range(generations):\n",
    "        # 1) select μ parents with coverage + elitism\n",
    "        parents = select_mu_with_coverage_and_elites(pool, mu=mu, k=tournament_k, elites=elites)\n",
    "\n",
    "        # 2) λ children (coverage pairing), self-adapt σ, mutate θ, eval\n",
    "        pairs = make_pairs_with_coverage(parents, num_pairs=lam)\n",
    "        child_thetas, child_sigmas = [], []\n",
    "        for p1, p2 in pairs:\n",
    "            th = crossover_pointwise(p1[\"theta\"], p2[\"theta\"], rate=cx_rate)\n",
    "            # geometric mean of parent sigmas\n",
    "            sig_par = max(1e-8, float(np.sqrt(p1[\"sigma\"] * p2[\"sigma\"])))\n",
    "            sig_child = mutate_sigma(sig_par, tau=tau_sigma)\n",
    "            th = mutate_with_sigma(th, sigma=sig_child, rate=mut_rate)\n",
    "            child_thetas.append(th); child_sigmas.append(sig_child)\n",
    "\n",
    "        children = _parallel_eval(child_thetas, N, T, dt, alpha_m, n_jobs=n_jobs)\n",
    "        for z, s in zip(children, child_sigmas):\n",
    "            z[\"sigma\"] = float(s)\n",
    "\n",
    "        # 3) next pool = μ parents + λ children\n",
    "        pool = parents + children\n",
    "        champ = max(pool, key=lambda z: z[\"fitness\"])\n",
    "        print(f\"Gen {g+1}/{generations} | best={champ['fitness']:.3f} | σ≈{champ['sigma']:.3g}\")\n",
    "\n",
    "        # 4) visualize champion (simulate once to get A)\n",
    "        if show:\n",
    "            champ = max(pool, key=lambda z: z[\"fitness\"])\n",
    "            viz = CEM1Layer(N, T, dt, alpha_m); viz.theta = dict(champ[\"theta\"]); viz.simulate()\n",
    "            plt.clf()\n",
    "            plt.imshow(viz.A, aspect='auto', origin='lower')\n",
    "            plt.title(f\"Gen {g+1}/{generations} | best={champ['fitness']:.3f} | σ≈{champ['sigma']:.3g}\")\n",
    "            plt.xlabel(\"i (cells)\"); plt.ylabel(\"t (steps)\")\n",
    "            plt.pause(0.001)\n",
    "\n",
    "    if show: plt.show(block=False)\n",
    "\n",
    "    # rebuild final CEMs (once) for return; sort best→worst\n",
    "    final = []\n",
    "    for z in pool:\n",
    "        c = CEM1Layer(N, T, dt, alpha_m); c.theta = dict(z[\"theta\"]); c.compute_fitness()\n",
    "        c.sigma = z[\"sigma\"]\n",
    "        final.append(c)\n",
    "    final.sort(key=lambda c: c.fitness, reverse=True)\n",
    "    return final\n"
   ],
   "id": "7ce74b73522624f8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:16:38.170735Z",
     "start_time": "2025-09-30T21:05:10.028735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "final_pop = mu_plus_lambda_ea_parallel_sa(\n",
    "    N=1000, T=1000, dt=0.3, alpha_m=0.05,\n",
    "    mu=50, lam=100, generations=100,\n",
    "    tournament_k=2, cx_rate=1.0,\n",
    "    mut_rate=0.05, base_sigma=.1, tau_sigma=.1,\n",
    "    elites=2, n_jobs=10, show=False, seed=33)\n"
   ],
   "id": "d4668b247a781da4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 1/100 | best=2.250 | σ≈0.116\n",
      "Gen 2/100 | best=2.250 | σ≈0.0951\n",
      "Gen 3/100 | best=2.250 | σ≈0.103\n",
      "Gen 4/100 | best=2.250 | σ≈0.0948\n",
      "Gen 5/100 | best=2.250 | σ≈0.0948\n",
      "Gen 6/100 | best=2.251 | σ≈0.0926\n",
      "Gen 7/100 | best=2.251 | σ≈0.0926\n",
      "Gen 8/100 | best=2.251 | σ≈0.0891\n",
      "Gen 9/100 | best=2.251 | σ≈0.0891\n",
      "Gen 10/100 | best=2.251 | σ≈0.0891\n",
      "Gen 11/100 | best=2.251 | σ≈0.0891\n",
      "Gen 12/100 | best=2.251 | σ≈0.0891\n",
      "Gen 13/100 | best=2.251 | σ≈0.0891\n",
      "Gen 14/100 | best=2.251 | σ≈0.0891\n",
      "Gen 15/100 | best=2.251 | σ≈0.103\n",
      "Gen 16/100 | best=2.251 | σ≈0.103\n",
      "Gen 17/100 | best=2.251 | σ≈0.103\n",
      "Gen 18/100 | best=2.251 | σ≈0.103\n",
      "Gen 19/100 | best=2.251 | σ≈0.103\n",
      "Gen 20/100 | best=2.251 | σ≈0.103\n",
      "Gen 21/100 | best=2.251 | σ≈0.103\n",
      "Gen 22/100 | best=2.251 | σ≈0.103\n",
      "Gen 23/100 | best=2.251 | σ≈0.103\n",
      "Gen 24/100 | best=2.251 | σ≈0.103\n",
      "Gen 25/100 | best=2.251 | σ≈0.103\n",
      "Gen 26/100 | best=2.251 | σ≈0.103\n",
      "Gen 27/100 | best=2.251 | σ≈0.103\n",
      "Gen 28/100 | best=2.251 | σ≈0.101\n",
      "Gen 29/100 | best=2.251 | σ≈0.101\n",
      "Gen 30/100 | best=2.251 | σ≈0.101\n",
      "Gen 31/100 | best=2.251 | σ≈0.101\n",
      "Gen 32/100 | best=2.251 | σ≈0.101\n",
      "Gen 33/100 | best=2.251 | σ≈0.101\n",
      "Gen 34/100 | best=2.251 | σ≈0.101\n",
      "Gen 35/100 | best=2.251 | σ≈0.101\n",
      "Gen 36/100 | best=2.251 | σ≈0.101\n",
      "Gen 37/100 | best=2.251 | σ≈0.101\n",
      "Gen 38/100 | best=2.251 | σ≈0.101\n",
      "Gen 39/100 | best=2.251 | σ≈0.101\n",
      "Gen 40/100 | best=2.251 | σ≈0.101\n",
      "Gen 41/100 | best=2.251 | σ≈0.101\n",
      "Gen 42/100 | best=2.251 | σ≈0.101\n",
      "Gen 43/100 | best=2.251 | σ≈0.101\n",
      "Gen 44/100 | best=2.251 | σ≈0.101\n",
      "Gen 45/100 | best=2.251 | σ≈0.101\n",
      "Gen 46/100 | best=2.251 | σ≈0.101\n",
      "Gen 47/100 | best=2.251 | σ≈0.101\n",
      "Gen 48/100 | best=2.251 | σ≈0.101\n",
      "Gen 49/100 | best=2.251 | σ≈0.101\n",
      "Gen 50/100 | best=2.251 | σ≈0.101\n",
      "Gen 51/100 | best=2.251 | σ≈0.101\n",
      "Gen 52/100 | best=2.251 | σ≈0.101\n",
      "Gen 53/100 | best=2.251 | σ≈0.101\n",
      "Gen 54/100 | best=2.251 | σ≈0.101\n",
      "Gen 55/100 | best=2.251 | σ≈0.101\n",
      "Gen 56/100 | best=2.251 | σ≈0.101\n",
      "Gen 57/100 | best=2.251 | σ≈0.101\n",
      "Gen 58/100 | best=2.251 | σ≈0.101\n",
      "Gen 59/100 | best=2.251 | σ≈0.101\n",
      "Gen 60/100 | best=2.251 | σ≈0.101\n",
      "Gen 61/100 | best=2.251 | σ≈0.101\n",
      "Gen 62/100 | best=2.251 | σ≈0.101\n",
      "Gen 63/100 | best=2.251 | σ≈0.101\n",
      "Gen 64/100 | best=2.251 | σ≈0.101\n",
      "Gen 65/100 | best=2.251 | σ≈0.101\n",
      "Gen 66/100 | best=2.251 | σ≈0.101\n",
      "Gen 67/100 | best=2.251 | σ≈0.101\n",
      "Gen 68/100 | best=2.251 | σ≈0.101\n",
      "Gen 69/100 | best=2.251 | σ≈0.101\n",
      "Gen 70/100 | best=2.251 | σ≈0.101\n",
      "Gen 71/100 | best=2.251 | σ≈0.101\n",
      "Gen 72/100 | best=2.251 | σ≈0.101\n",
      "Gen 73/100 | best=2.251 | σ≈0.101\n",
      "Gen 74/100 | best=2.251 | σ≈0.101\n",
      "Gen 75/100 | best=2.251 | σ≈0.101\n",
      "Gen 76/100 | best=2.251 | σ≈0.101\n",
      "Gen 77/100 | best=2.251 | σ≈0.101\n",
      "Gen 78/100 | best=2.251 | σ≈0.101\n",
      "Gen 79/100 | best=2.251 | σ≈0.101\n",
      "Gen 80/100 | best=2.251 | σ≈0.101\n",
      "Gen 81/100 | best=2.251 | σ≈0.101\n",
      "Gen 82/100 | best=2.251 | σ≈0.101\n",
      "Gen 83/100 | best=2.251 | σ≈0.101\n",
      "Gen 84/100 | best=2.251 | σ≈0.101\n",
      "Gen 85/100 | best=2.251 | σ≈0.101\n",
      "Gen 86/100 | best=2.251 | σ≈0.101\n",
      "Gen 87/100 | best=2.251 | σ≈0.101\n",
      "Gen 88/100 | best=2.251 | σ≈0.101\n",
      "Gen 89/100 | best=2.251 | σ≈0.101\n",
      "Gen 90/100 | best=2.251 | σ≈0.101\n",
      "Gen 91/100 | best=2.251 | σ≈0.101\n",
      "Gen 92/100 | best=2.251 | σ≈0.101\n",
      "Gen 93/100 | best=2.251 | σ≈0.101\n",
      "Gen 94/100 | best=2.251 | σ≈0.101\n",
      "Gen 95/100 | best=2.251 | σ≈0.101\n",
      "Gen 96/100 | best=2.251 | σ≈0.101\n",
      "Gen 97/100 | best=2.251 | σ≈0.101\n",
      "Gen 98/100 | best=2.251 | σ≈0.101\n",
      "Gen 99/100 | best=2.251 | σ≈0.101\n",
      "Gen 100/100 | best=2.251 | σ≈0.101\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:16:55.826619Z",
     "start_time": "2025-09-30T21:16:55.749709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_power_law_fit(plot_data):\n",
    "    \"\"\"\n",
    "    Generates a log-log plot of the avalanche size distribution and its power-law fit.\n",
    "\n",
    "    Args:\n",
    "        plot_data (dict): The dictionary of results from compute_criticality_metric_numpy.\n",
    "    \"\"\"\n",
    "    if plot_data is None or not plot_data.get('fit_successful'):\n",
    "        print(\"Fit was not successful or no data provided. Cannot generate plot.\")\n",
    "        return\n",
    "\n",
    "    alpha = plot_data['alpha']\n",
    "    xmin = plot_data['xmin']\n",
    "    ks_dist = plot_data['ks_distance']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot the empirical data (avalanche size distribution)\n",
    "    ax.scatter(plot_data['empirical_pdf_x'], plot_data['empirical_pdf_y'],\n",
    "               color='blue', alpha=0.7, label='Empirical Data (PDF)')\n",
    "\n",
    "    # Plot the fitted power-law line\n",
    "    ax.plot(plot_data['fit_pdf_x'], plot_data['fit_pdf_y'],\n",
    "            color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Power-Law Fit (α={alpha:.2f})')\n",
    "\n",
    "    # Add a vertical line to indicate xmin\n",
    "    ax.axvline(xmin, color='gray', linestyle=':', linewidth=2,\n",
    "               label=f'xmin = {xmin}')\n",
    "\n",
    "    # --- Formatting the plot ---\n",
    "    ax.set_xlabel('Avalanche Size (s)')\n",
    "    ax.set_ylabel('Probability P(s)')\n",
    "    ax.set_title(f'Power-Law Distribution of Avalanche Sizes (KS={ks_dist:.3f})')\n",
    "\n",
    "    # Set log-log scale, which is essential for visualizing power laws\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_paper_criticiality_fits(plot_data_list):\n",
    "    \"\"\"\n",
    "    Generates a 2x3 grid of log-log plots to replicate Figure 4 from the paper,\n",
    "    using the data returned by compute_criticality_from_paper.\n",
    "\n",
    "    Args:\n",
    "        plot_data_list (list): A list of 6 dictionaries, one for each distribution.\n",
    "    \"\"\"\n",
    "    if plot_data_list is None or len(plot_data_list) != 6:\n",
    "        print(\"Invalid plot data provided. Cannot generate plots.\")\n",
    "        return\n",
    "\n",
    "    titles = [\n",
    "        'Avalanche size of state 0', 'Avalanche duration of state 0', 'Avalanche area of state 0',\n",
    "        'Avalanche size of state 1', 'Avalanche duration of state 1', 'Avalanche area of state 1'\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        data = plot_data_list[i]\n",
    "\n",
    "        if data['empirical_x'] is None:\n",
    "            ax.set_title(f\"{titles[i]}\\n(No data)\")\n",
    "            continue\n",
    "\n",
    "        # Plot the empirical data (blue line in the paper)\n",
    "        ax.plot(data['empirical_x'], data['empirical_y'], 'b-', marker='o', markersize=4,\n",
    "                label='Empirical Data (PDF)')\n",
    "\n",
    "        # Plot the fitted line (dashed black line in the paper)\n",
    "        if data['fit_x'] is not None:\n",
    "            ax.plot(data['fit_x'], data['fit_y'], 'k--', linewidth=2,\n",
    "                    label=f'Fit (α={data[\"alpha\"]:.2f})')\n",
    "\n",
    "        # --- Formatting ---\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('P(x)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        ax.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "fit_res = final_pop[0].fit_res\n",
    "#plot_paper_criticiality_fits(fit_res)\n",
    "a =  final_pop[0].A\n",
    "print(a.min(), a.max())\n",
    "plt.figure(2)\n",
    "plt.imshow(a[:1000,:1000 ]  >0  , cmap='gray')\n",
    "print(final_pop[0].theta)"
   ],
   "id": "b284409e210d7e70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11921695339584475 1.0\n",
      "{'k0': 0.06060982936177303, 'k1': -0.13017837232543342, 'k2': -0.6641433593772232, 'k3': -0.06741873149928485, 'k4': 0.1270049630986631, 'k5': 0.13704476653080488, 'k6': -0.09803916570546584}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:16:38.667421303Z",
     "start_time": "2025-09-30T21:01:04.069451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "Theta = Dict[str, float]\n",
    "def save_theta(theta: Theta, path: str):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(theta, f, indent=2)\n",
    "\n",
    "save_theta(final_pop[0].theta, 'Crit_pretraining/1Channel_CEM/pretrained2.json')"
   ],
   "id": "2265919b7cb7d4d0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T21:16:38.668269718Z",
     "start_time": "2025-09-30T21:01:04.114016Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3ea4e0e937a45375",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
